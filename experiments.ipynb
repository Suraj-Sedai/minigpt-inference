{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP28hSjqT5/LjvYxryvcFT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/kv-cache-transformer/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT-Inference [KV-CACHE TRANSFORMER]\n",
        "\n",
        "**A From-Scratch Transformer Inference Engine**\n",
        "\n",
        "MiniGPT-Inference is a from-scratch, production-grade Transformer inference engine designed to execute autoregressive decoding efficiently using Keyâ€“Value (KV) caching, incremental decoding, and batched generation.\n",
        "\n",
        "Unlike training-focused implementations, this project centers on inference-time systems engineering, emphasizing:\n",
        "- Computational complexity reduction\n",
        "- Memory efficiency\n",
        "- Deterministic correctness\n",
        "- Measurable performance gains\n",
        "\n",
        "The system is architected to reflect how modern large language models (LLMs) are served in real-world environments."
      ],
      "metadata": {
        "id": "bq03RwLpRE-9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b947b0"
      },
      "source": [
        "# Project Setup: Model Architecture Configuration\n",
        "\n",
        "This section outlines the foundational configuration for our model. The `ModelConfig` dataclass is used to define key architectural hyperparameters, centralizing them for clarity, reusability, and ease of modification.\n",
        "\n",
        "The parameters included in `ModelConfig` are typically found in transformer-based models and include:\n",
        "*   `vocab_size`: The size of the vocabulary, representing the number of unique tokens the model can process.\n",
        "*   `n_layers`: The number of transformer layers or blocks within the model's architecture.\n",
        "*   `n_heads`: The number of attention heads used in the multi-head attention mechanism within each transformer layer.\n",
        "*   `d_model`: The dimensionality of the model's embeddings and internal representations.\n",
        "*   `block_size`: The maximum sequence length or context window that the model can process at once.\n",
        "*   `dropout`: The dropout rate applied for regularization to prevent overfitting.\n",
        "\n",
        "By using a `dataclass`, we achieve immutability for the configuration once defined (due to `frozen=True`), which helps prevent accidental changes to the model's blueprint during its lifecycle. The `head_dim` property is also derived to ensure `d_model` is divisible by `n_heads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a299e1"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)#prevents accidental mutation\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_model: int\n",
        "    block_size: int\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a66fe78"
      },
      "source": [
        "### Embedding Layers: Token and Positional\n",
        "\n",
        "Transformer models rely on embedding layers to convert discrete input tokens into continuous vector representations, capturing both semantic meaning and sequential order.\n",
        "\n",
        "#### `TokenEmbedding`\n",
        "\n",
        "This layer converts numerical token IDs into dense vectors. Each unique token in the vocabulary is mapped to a `d_model`-dimensional vector, allowing the model to process linguistic information. This is achieved using `torch.nn.Embedding`, where `vocab_size` determines the number of unique tokens and `d_model` is the dimensionality of the embedding vectors.\n",
        "\n",
        "#### `PositionalEmbedding`\n",
        "\n",
        "Since Transformers process sequences in parallel and lack an inherent understanding of token order, positional embeddings are crucial. This layer provides a vector representation for each position within the input sequence up to `block_size`. These positional vectors are added to the token embeddings, injecting information about the relative or absolute position of each token in the sequence. Like token embeddings, it uses `torch.nn.Embedding` to map position IDs to `d_model`-dimensional vectors.\n",
        "\n",
        "**Key Concept:** The final input to the Transformer encoder is typically the sum of the token embedding and its corresponding positional embedding. This combined representation allows the model to differentiate between identical tokens appearing at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ddd496"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming ModelConfig is defined in model.config or already imported\n",
        "# from model.config import ModelConfig # Uncomment if not already imported\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, T)\n",
        "        returns:   (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(token_ids)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.block_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, position_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        position_ids: (T) or (B, T)\n",
        "        returns:      (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(position_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a58094"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "Scaled Dot-Product Attention is a fundamental component of the Transformer architecture, designed to efficiently compute attention weights. It takes three inputs: a query matrix (Q), a key matrix (K), and a value matrix (V). The core idea is to calculate a similarity score between the queries and keys, scale these scores, and then use them to weigh the values.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Similarity Calculation:** The attention scores are computed by taking the dot product of the query and key matrices. This measures how relevant each key is to each query.\n",
        "2.  **Scaling:** The scores are then divided by the square root of the dimension of the keys (`d_k`). This scaling factor is crucial for preventing the dot products from becoming too large, especially with high `d_k` values, which can push the softmax function into regions with extremely small gradients, hindering training.\n",
        "3.  **Masking (Optional):** If a mask is provided, typically for causality (to prevent attention to future tokens in sequence generation) or padding (to ignore non-existent tokens), the masked positions are set to a very small negative number (e.g., `-inf`). This ensures that after the softmax operation, these positions will have an attention weight of approximately zero.\n",
        "4.  **Softmax:** A softmax function is applied to the scaled scores to obtain attention weights. This normalizes the scores such that they sum to 1, representing a probability distribution over the values.\n",
        "5.  **Weighted Sum:** Finally, these attention weights are multiplied by the value matrix (V). This creates a weighted sum of the values, where the weight assigned to each value is determined by its relevance to the query.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The Scaled Dot-Product Attention mechanism is mathematically expressed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "-   $Q$ is the Query matrix.\n",
        "-   $K$ is the Key matrix.\n",
        "-   $V$ is the Value matrix.\n",
        "-   $d_k$ is the dimension of the key vectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.scale = 1.0 / math.sqrt(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        q, k, v: (B, T, D)\n",
        "        mask:    (T, T) or (B, T, T)\n",
        "        return:  (B, T, D)\n",
        "        \"\"\"\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, v)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "rYFuFZOw_EP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4a41f5"
      },
      "source": [
        "### Causal Self-Attention\n",
        "\n",
        "`CausalSelfAttention` is a crucial component in transformer-based autoregressive models, such as GPT (Generative Pre-trained Transformer). It extends the concept of `ScaledDotProductAttention` by ensuring that during sequence generation, each token can only attend to previous tokens and itself, not future tokens. This is vital for tasks like language modeling where predicting the next word depends only on the words that have already occurred.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   It takes a `ModelConfig` object, which defines parameters like the number of attention heads (`n_heads`), the dimensionality of each head (`head_dim`), and the model's total dimension (`d_model`).\n",
        "    *   `self.scale`: A scaling factor `1 / sqrt(head_dim)` is calculated, which is standard for scaled dot-product attention to prevent large dot products from pushing the softmax into regions with tiny gradients.\n",
        "    *   `self.qkv_proj`: A linear projection layer that transforms the input `x` (with shape `(B, T, D)`) into Query (Q), Key (K), and Value (V) matrices. It outputs `3 * d_model` dimensions, which are then split into `d_model` for Q, K, and V respectively.\n",
        "    *   `self.out_proj`: Another linear projection layer that takes the concatenated output from all attention heads and projects it back to the original `d_model` dimension.\n",
        "    *   `self.causal_mask`: A lower triangular matrix (e.g., `[[1,0,0],[1,1,0],[1,1,1]]`) is created. This mask is used to block attention to future tokens. It's registered as a buffer, meaning it's part of the model's state but not a trainable parameter.\n",
        "\n",
        "2.  **Forward Pass (`forward`)**:\n",
        "    *   **Input**: `x` with shape `(B, T, D)`, where `B` is batch size, `T` is sequence length, and `D` is `d_model`.\n",
        "    *   **QKV Projections**: The input `x` is passed through `self.qkv_proj` to get a combined `qkv` tensor. This `qkv` tensor is then split into `q`, `k`, and `v` tensors, each of shape `(B, T, D)`.\n",
        "    *   **Multi-Head Reshaping**: Each `q`, `k`, and `v` tensor is reshaped to `(B, n_heads, T, head_dim)`. This involves splitting the `d_model` dimension into `n_heads` separate heads, each with `head_dim` dimensions. The `transpose(1, 2)` operation rearranges the dimensions to put the heads dimension before the sequence length dimension, which is standard for multi-head attention computations.\n",
        "    *   **Attention Scores Calculation**: The core attention mechanism is computed:\n",
        "        $$\\text{scores} = (Q K^T) / \\sqrt{d_k}$$\n",
        "        Here, `q` and `k` (reshaped `(B, n_heads, T, head_dim)`) are multiplied (`torch.matmul`) to get the similarity scores. `k.transpose(-2, -1)` transposes the last two dimensions of `k`, effectively performing $K^T$. The result is then scaled by `self.scale` (`1 / sqrt(head_dim)`).\n",
        "        The shape of `scores` is `(B, n_heads, T, T)`.\n",
        "    *   **Causal Masking**: The `causal_mask` (a lower triangular matrix) is applied. For each position `i` in the sequence, the mask ensures that the attention scores for positions `j > i` (future tokens) are set to negative infinity. This means that after the softmax, these future positions will have an attention weight of zero, effectively preventing a token from attending to future tokens.\n",
        "        `scores = scores.masked_fill(mask == 0, float(\"-inf\"))`\n",
        "    *   **Softmax**: A `softmax` function is applied to the scores along the last dimension (`dim=-1`) to obtain attention `weights`. This normalizes the scores so they sum to 1 for each query, representing a probability distribution over the values.\n",
        "        `weights = torch.softmax(scores, dim=-1)`\n",
        "    *   **Weighted Sum of Values**: The attention `weights` are then multiplied by the `v` (value) tensor (`torch.matmul`). This produces the weighted sum of values, where tokens with higher attention weights contribute more to the output.\n",
        "        `out = torch.matmul(weights, v)`\n",
        "        The shape of `out` is `(B, n_heads, T, head_dim)`.\n",
        "    *   **Merge Heads**: The `out` tensor is reshaped back to `(B, T, D)`. This involves transposing the dimensions back and then concatenating the outputs from all heads (`contiguous().view(B, T, D)`).\n",
        "    *   **Output Projection**: Finally, the merged output is passed through `self.out_proj` to produce the final output of the self-attention layer. This projection allows the model to learn a linear transformation on the combined information from all attention heads.\n",
        "\n",
        "**Mathematical Intuition:**\n",
        "\n",
        "The causal self-attention mechanism fundamentally implements the following operation for each head:\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T + M}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Where:\n",
        "*   $Q$, $K$, $V$ are the Query, Key, and Value matrices for a single head.\n",
        "*   $d_k$ is `head_dim`, the dimensionality of the key vectors.\n",
        "*   $M$ is the causal mask, where $M_{ij} = 0$ if $i \\ge j$ (past and current tokens) and $M_{ij} = -\\infty$ if $i < j$ (future tokens). This effectively makes the attention weights to future tokens zero.\n",
        "\n",
        "The multi-head aspect involves performing this attention operation `n_heads` times in parallel with different linear projections for each head, and then concatenating and linearly projecting their outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        # causal mask (registered as buffer, not parameter)\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        return: (B, T, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # apply causal mask\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v)\n",
        "\n",
        "        # merge heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "-HCKn1q5iagx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9801871b"
      },
      "source": [
        "### KVCache and CachedCausalSelfAttention: Optimizing Inference with KV Caching\n",
        "\n",
        "To enhance the efficiency of autoregressive decoding in Transformer models, especially during inference, Key-Value (KV) caching is employed. This technique avoids redundant re-computation of keys and values for previously processed tokens, significantly speeding up generation. The `KVCache` class manages this storage, and `CachedCausalSelfAttention` utilizes it for incremental token processing.\n",
        "\n",
        "#### 1. `KVCache` Class\n",
        "\n",
        "The `KVCache` class is a simple container designed to store the keys (K) and values (V) computed during the self-attention mechanism across multiple decoding steps. This cache allows subsequent tokens to attend to the full historical context without re-calculating the K and V matrices for past tokens.\n",
        "\n",
        "**Operational Breakdown:**\n",
        "\n",
        "*   **`__init__(self)`**: Initializes an empty cache by setting `self.keys` and `self.values` to `None`. This state indicates that no keys or values have been stored yet.\n",
        "\n",
        "*   **`append(self, k_new: torch.Tensor, v_new: torch.Tensor)`**: This method adds new key and value tensors to the cache. It expects `k_new` and `v_new` to have the shape `(B, H, 1, Dh)`, representing the keys and values for the current token across batches and attention heads.\n",
        "    *   If the cache is empty (`self.keys` is `None`), the new keys and values become the initial content of the cache.\n",
        "    *   If the cache already contains data, `k_new` and `v_new` are concatenated with the existing `self.keys` and `self.values` along the sequence length dimension (dimension 2). This effectively appends the current token's K and V to the historical sequence.\n",
        "\n",
        "*   **`reset(self)`**: Clears the cache by setting `self.keys` and `self.values` back to `None`. This is typically used to prepare the cache for a new generation sequence.\n",
        "\n",
        "#### 2. `CachedCausalSelfAttention` Class\n",
        "\n",
        "The `CachedCausalSelfAttention` module is a specialized version of the `CausalSelfAttention` designed for efficient token-by-token generation (inference) by leveraging the `KVCache`.\n",
        "\n",
        "**Key Differences from `CausalSelfAttention`:**\n",
        "*   It processes input `x_t` with a sequence length `T=1` (a single token at a time).\n",
        "*   It takes a `kv_cache` object as an argument to store and retrieve past keys and values.\n",
        "*   It *implicitly* handles causality by only attending to the `k_full` and `v_full` retrieved from the cache, which by its nature only contains past and current tokens.\n",
        "\n",
        "**Operational Breakdown (`forward` method):**\n",
        "\n",
        "*   **Input**: `x_t` with shape `(B, 1, D)` (a single token per batch) and `kv_cache`.\n",
        "\n",
        "*   **Assertion**: `assert T == 1, \"Cached attention expects exactly one token\"` ensures that the module is used for incremental decoding.\n",
        "\n",
        "*   **QKV Projections**: Similar to `CausalSelfAttention`, `x_t` is projected into query `q`, key `k`, and value `v` tensors for the *current* token.\n",
        "\n",
        "*   **Multi-Head Reshaping**: `q`, `k`, and `v` are reshaped to `(B, n_heads, 1, head_dim)` to prepare for multi-head attention.\n",
        "\n",
        "*   **Cache Append**: The newly computed `k` and `v` for the current token are appended to the `kv_cache` using `kv_cache.append(k, v)`. The cache now holds the keys and values for *all* tokens processed so far in the current sequence.\n",
        "\n",
        "*   **Retrieve Full Cache**: The complete historical `keys` (`k_full`) and `values` (`v_full`) are retrieved from the `kv_cache`. These tensors will have shape `(B, H, T_total, Dh)`, where `T_total` is the current length of the generated sequence.\n",
        "\n",
        "*   **Attention Score Calculation**: The query `q` (current token's query) is used to compute attention scores against `k_full` (all past and current keys). This ensures that the current token attends to the entire context generated so far.\n",
        "    $$\\text{scores} = (Q_{\\text{current}} K_{\\text{full}}^T) / \\sqrt{d_k}$$\n",
        "    The `scores` tensor will have shape `(B, n_heads, 1, T_total)`.\n",
        "\n",
        "*   **Softmax and Weighted Sum**: `softmax` is applied to the scores, and the resulting attention weights are multiplied by `v_full` to produce the output `out` for the current token. This `out` tensor effectively summarizes the information from `v_full` relevant to the current `q`.\n",
        "\n",
        "*   **Merge Heads and Output Projection**: The `out` tensor is reshaped back to `(B, 1, D)` and then passed through `self.out_proj` to yield the final output for the current token.\n",
        "\n",
        "**Mathematical Intuition for Cached Self-Attention:**\n",
        "\n",
        "The core attention computation within `CachedCausalSelfAttention` can be viewed as:\n",
        "\n",
        "$$ \\text{Attention}(\\text{token}_t) = \\text{softmax}\\left(\\frac{Q_t \\cdot K_{\\le t}^T}{\\sqrt{d_k}}\\right) \\cdot V_{\\le t} $$\n",
        "\n",
        "Where:\n",
        "*   $Q_t$ is the Query vector for the current token at position $t$.\n",
        "*   $K_{\\le t}$ represents the concatenated Key matrix containing keys for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.keys`).\n",
        "*   $V_{\\le t}$ represents the concatenated Value matrix containing values for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.values`).\n",
        "*   $d_k$ is the `head_dim`, the dimensionality of the key vectors.\n",
        "\n",
        "In this formulation, the causal masking that explicitly masks future tokens in `CausalSelfAttention` is implicitly handled. By only storing and using keys and values from tokens up to the current position ($K_{\\le t}$ and $V_{\\le t}$), the model naturally prevents attending to future information. The KV cache makes this process highly efficient by avoiding re-computation of $K_{\\le t}$ and $V_{\\le t}$ at each step; instead, it simply appends the new $K_t$ and $V_t$ to the existing cache."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache:\n",
        "    def __init__(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n",
        "\n",
        "    def append(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        \"\"\"\n",
        "        k_new, v_new: (B, H, 1, Dh)\n",
        "        \"\"\"\n",
        "        if self.keys is None:\n",
        "            self.keys = k_new\n",
        "            self.values = v_new\n",
        "        else:\n",
        "            self.keys = torch.cat([self.keys, k_new], dim=2)\n",
        "            self.values = torch.cat([self.values, v_new], dim=2)\n",
        "\n",
        "    def reset(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n"
      ],
      "metadata": {
        "id": "jrBBbZLPmWnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedCausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, kv_cache) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_t: (B, 1, D)\n",
        "        kv_cache: KVCache\n",
        "        return: (B, 1, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x_t.shape\n",
        "        assert T == 1, \"Cached attention expects exactly one token\"\n",
        "\n",
        "        qkv = self.qkv_proj(x_t)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # append new K, V to cache\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        # retrieve full cached K, V\n",
        "        k_full = kv_cache.keys     # (B, H, T_total, Dh)\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = torch.matmul(q, k_full.transpose(-2, -1)) * self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "G-hcqd0VmZig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc48e58d"
      },
      "source": [
        "### FeedForward Layer\n",
        "\n",
        "In the Transformer architecture, the FeedForward layer (also known as the Position-wise Feed-Forward Network or FFN) is applied independently to each position in the sequence. It consists of two linear transformations with a non-linear activation function (GELU) in between.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **First Linear Layer (`self.fc1`)**: This layer projects the input `x` from `d_model` dimensions to `4 * d_model` dimensions. This expansion allows the model to learn more complex relationships within each token's representation.\n",
        "2.  **Activation Function (`self.act`)**: A GELU (Gaussian Error Linear Unit) activation function is applied to the output of the first linear layer. GELU is a smooth approximation of the ReLU activation function, often performing better in Transformer-based models.\n",
        "3.  **Second Linear Layer (`self.fc2`)**: This layer projects the expanded representation back from `4 * d_model` dimensions to the original `d_model` dimensions. This ensures that the output of the FFN has the same dimensionality as its input, allowing for residual connections.\n",
        "\n",
        "This layer processes each position identically but independently, meaning the same weights are used for all positions, but each position gets its own distinct computation.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The FeedForward layer can be mathematically expressed as:\n",
        "\n",
        "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "Where:\n",
        "*   $x$ is the input to the FeedForward network, typically the output of the self-attention sub-layer.\n",
        "*   $W_1$ and $b_1$ are the weights and biases of the first linear transformation (from `d_model` to `4 * d_model`).\n",
        "*   $W_2$ and $b_2$ are the weights and biases of the second linear transformation (from `4 * d_model` to `d_model`).\n",
        "*   $\\text{GELU}$ is the Gaussian Error Linear Unit activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from model.config import ModelConfig\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
        "        self.fc2 = nn.Linear(4 * config.d_model, config.d_model)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc2(self.act(self.fc1(x)))"
      ],
      "metadata": {
        "id": "ZkghrdjFmle1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8202b0"
      },
      "source": [
        "### TransformerBlock\n",
        "\n",
        "The `TransformerBlock` is the core building block of the Transformer's encoder and decoder. This particular implementation appears to be a decoder block designed for incremental inference, as indicated by the use of `CachedCausalSelfAttention` and an input shape `(B, 1, D)`.\n",
        "\n",
        "Each `TransformerBlock` consists of two main sub-layers, each followed by a residual connection and layer normalization:\n",
        "\n",
        "1.  **Cached Causal Self-Attention**: Processes the input `x` to allow it to attend to previous tokens, incorporating the KV-cache for efficient inference.\n",
        "2.  **Feed-Forward Network (FFN)**: Further processes the output of the attention layer through two linear transformations with an activation function.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "*   **`self.ln1` (Layer Normalization)**: Applied before the attention sub-layer. Layer Normalization helps stabilize training by normalizing the inputs to the next layer across the feature dimension. It ensures that the mean and variance of the inputs are consistent.\n",
        "*   **`self.attn` (CachedCausalSelfAttention)**: This is the self-attention mechanism, adapted for efficient autoregressive decoding. It takes the layer-normalized input `self.ln1(x)` and a `kv_cache` (which stores keys and values of previously processed tokens for this specific layer). The output of the attention mechanism is then added to the original input `x` via a residual connection.\n",
        "*   **`self.ln2` (Layer Normalization)**: Applied before the Feed-Forward Network. Similar to `self.ln1`, it normalizes the input to the FFN.\n",
        "*   **`self.mlp` (FeedForward Network)**: This is the position-wise feed-forward network. It takes the layer-normalized output of the attention sub-layer `self.ln2(x)` and processes it. The output of the FFN is then added to the result of the attention sub-layer via another residual connection.\n",
        "\n",
        "**Forward Pass Logic (`forward` method):**\n",
        "\n",
        "The `forward` method implements the following sequence of operations:\n",
        "1.  **Attention Sub-layer**: The input `x` is first normalized by `self.ln1`. This normalized input is then passed to the `self.attn` module along with the `kv_cache` for the current layer. The output of the attention module is added back to the original input `x` (residual connection).\n",
        "    *   `x = x + self.attn(self.ln1(x), kv_cache)`\n",
        "2.  **Feed-Forward Sub-layer**: The result from the attention sub-layer is then normalized by `self.ln2`. This normalized result is passed to the `self.mlp` module. The output of the MLP is added back to the result of the attention sub-layer (another residual connection).\n",
        "    *   `x = x + self.mlp(self.ln2(x))`\n",
        "3.  **Output**: The final result `x` is the output of the Transformer block.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "Let $x$ be the input to the Transformer block, and $x_{\\text{cache}}$ represent the `kv_cache` for the current layer.\n",
        "\n",
        "1.  **Layer Normalization 1**: $x' = \\text{LayerNorm}_1(x)$\n",
        "2.  **Cached Causal Self-Attention**: $x'' = \\text{CachedCausalSelfAttention}(x', x_{\\text{cache}})$\n",
        "3.  **Residual Connection 1**: $x_{\\text{attn}} = x + x''$\n",
        "4.  **Layer Normalization 2**: $x''' = \\text{LayerNorm}_2(x_{\\text{attn}})$\n",
        "5.  **Feed-Forward Network**: $x'''' = \\text{FeedForward}(x''')$\n",
        "6.  **Residual Connection 2**: $x_{\\text{output}} = x_{\\text{attn}} + x''''$\n",
        "\n",
        "Thus, the entire block's operation can be summarized as:\n",
        "\n",
        "$$ \\text{TransformerBlock}(x, x_{\\text{cache}}) = \\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}})) + \\text{FeedForward}(\\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}}))) $$\n",
        "\n",
        "This structure, often referred to as \"Pre-Normalization\" or \"Pre-LN\" Transformer, applies layer normalization before the self-attention and FFN sub-layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from model.config import ModelConfig\n",
        "from model.attention import CachedCausalSelfAttention\n",
        "from model.layers import FeedForward\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CachedCausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_cache) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, 1, D)\n",
        "        kv_cache: KVCache for this layer\n",
        "        \"\"\"\n",
        "        x = x + self.attn(self.ln1(x), kv_cache)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bZ2ZH_6xmpig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}