{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMAyLysaqWGF/nH5NtfV0tV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Sedai/minigpt-inference/blob/main/notebooks/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT-Inference [KV-CACHE TRANSFORMER]\n",
        "\n",
        "**A From-Scratch Transformer Inference Engine**\n",
        "\n",
        "MiniGPT-Inference is a from-scratch, production-grade Transformer inference engine designed to execute autoregressive decoding efficiently using Key–Value (KV) caching, incremental decoding, and batched generation.\n",
        "\n",
        "Unlike training-focused implementations, this project centers on inference-time systems engineering, emphasizing:\n",
        "- Computational complexity reduction\n",
        "- Memory efficiency\n",
        "- Deterministic correctness\n",
        "- Measurable performance gains\n",
        "\n",
        "The system is architected to reflect how modern large language models (LLMs) are served in real-world environments."
      ],
      "metadata": {
        "id": "bq03RwLpRE-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "### Project Goal: MiniGPT-Inference\n",
        "\n",
        "The MiniGPT-Inference project aims to build a Transformer inference engine from scratch, specifically optimized for efficient autoregressive decoding. The core design principles revolve around **computational complexity reduction**, **memory efficiency**, **deterministic correctness**, and **measurable performance gains**. This is achieved primarily through the implementation of Key-Value (KV) caching, incremental decoding, and batched generation, contrasting with training-focused Transformer implementations. The system is designed to mimic real-world LLM serving environments.\n",
        "\n",
        "### Analysis of KVCache Validation\n",
        "\n",
        "The `KVCache` validation test (cell `e36ef20a`) demonstrates the functionality of the KV cache's accumulation mechanism.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Step 0: keys shape = torch.Size([1, 2, 1, 4])\n",
        "    Step 1: keys shape = torch.Size([1, 2, 2, 4])\n",
        "    Step 2: keys shape = torch.Size([1, 2, 3, 4])\n",
        "    ```\n",
        "*   **Conclusion:** The output clearly shows that with each call to `cache.append(k, v)`, the sequence length dimension (the third dimension in the `keys` tensor, index 2) incrementally increases by one. Starting from `1`, it progresses to `2`, then `3`. This confirms that the `KVCache` class correctly accumulates the keys and values of successive tokens by concatenating them along the sequence length dimension, which is fundamental for efficient incremental decoding.\n",
        "\n",
        "### Analysis of Cached Attention Validation\n",
        "\n",
        "The `CachedCausalSelfAttention` test (cell `d79f4cb9`) validates the output shape consistency of the cached attention mechanism during incremental decoding.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Step 0: output shape = torch.Size([1, 1, 8])\n",
        "    Step 1: output shape = torch.Size([1, 1, 8])\n",
        "    Step 2: output shape = torch.Size([1, 1, 8])\n",
        "    ```\n",
        "*   **Conclusion:** The output shows that for each step of incremental decoding (where a single token `x` of shape `(1, 1, 8)` is passed), the `CachedCausalSelfAttention` module consistently produces an output of shape `(1, 1, 8)`. This confirms that the cached attention mechanism is correctly configured to process one token at a time while maintaining the expected output dimensionality, crucial for step-by-step autoregressive generation.\n",
        "\n",
        "### Analysis of TransformerBlock Validation\n",
        "\n",
        "The `TransformerBlock` test (cell `13817821`) examines the behavior of a single Transformer block in an incremental decoding setting.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Step 0: x shape = torch.Size([1, 1, 8]) cache length = 1\n",
        "    Step 1: x shape = torch.Size([1, 1, 8]) cache length = 2\n",
        "    Step 2: x shape = torch.Size([1, 1, 8]) cache length = 3\n",
        "    ```\n",
        "*   **Conclusion:** The validation confirms two critical aspects:\n",
        "    1.  **Shape Consistency:** The output `x` maintains its shape `(1, 1, 8)` across all steps, indicating that the residual connections, layer normalizations, and the `forward_cached` attention mechanism within the `TransformerBlock` correctly preserve the tensor's dimensions for single-token processing.\n",
        "    2.  **Cache Accumulation:** The `cache length` (referring to `cache.keys.size(2)`) correctly increases with each step, demonstrating that the `KVCache` associated with this block is being updated and accumulated as new tokens are processed. This verifies the integrity of the `TransformerBlock` in an incremental, cached setting.\n",
        "\n",
        "### Analysis of Full Model `forward_step` Validation\n",
        "\n",
        "The `MiniGPTInferenceModel`'s `forward_step` validation (cell `ceae381a`) verifies the model's ability to process individual tokens incrementally and manage its internal state.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Step 0: logits shape = torch.Size([1, 100]) position = 1\n",
        "    Step 1: logits shape = torch.Size([1, 100]) position = 2\n",
        "    Step 2: logits shape = torch.Size([1, 100]) position = 3\n",
        "    ```\n",
        "*   **Conclusion:** The test demonstrates that `model.forward_step()` correctly produces `logits` of shape `(1, 100)` (Batch size x Vocab size) for each single-token input. Crucially, the `model.position` counter increments accurately with each step, reflecting the growing sequence length. This confirms that the `MiniGPTInferenceModel` successfully processes tokens one-by-one, providing logits for the next token while maintaining the correct positional context.\n",
        "\n",
        "### Analysis of Minimal Generation Smoke Test\n",
        "\n",
        "The minimal generation smoke test (cell `a4b1dddd`) provides an initial check of the end-to-end `generate` function.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    torch.Size([1, 6])\n",
        "    ```\n",
        "*   **Conclusion:** Given an `input_ids` prompt of length `3` (`[[1, 2, 3]]`) and a request to generate `max_new_tokens=5`, the `generate` function produced an output tensor of shape `(1, 6)`. This means it correctly returned the last token of the prompt (`1` token) plus the `5` newly generated tokens, resulting in a total length of `6`. This confirms the basic functionality of the generation process, including feeding the prompt, incremental token generation, and concatenating the results.\n",
        "\n",
        "### Analysis of Equivalence Test\n",
        "\n",
        "The equivalence test (cell `CZz2sjfUC_UY` and `1018e165`) compares the outputs of the cached and non-cached attention mechanisms.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Max difference: 2.384185791015625e-07\n",
        "    ```\n",
        "*   **Conclusion:** The observed maximum difference between the cached and full forward pass logits is `2.384185791015625e-07`. This extremely small value is well within the typical range of floating-point precision errors. This result provides strong evidence that the KV caching mechanism, while significantly optimizing for speed during inference, maintains **deterministic correctness** and is functionally equivalent to performing a full forward pass on the entire sequence at each step. This is a critical validation point for the project's goal of ensuring correctness.\n",
        "\n",
        "### Analysis of Sampling (Top-k/Top-p) Tests\n",
        "\n",
        "The various sampling tests (cells `3KXUUPAtMuxB`, `e0Er4lYHNS-R`, `Q8J_XLv4Na7H`, `DRw2FcC-NubE`, `rAgh084hN5Kt`) confirm the robustness and configurability of the implemented sampling strategies.\n",
        "\n",
        "*   **Deterministic Sanity Check (Greedy Works):**\n",
        "    *   **Output:** `tensor([[ 3, 42, 51, 95, 82, 51]])`\n",
        "    *   **Conclusion:** When `temperature=0.0`, the `generate1` function consistently produces the same sequence, confirming its deterministic greedy sampling behavior (always picking the highest probability token).\n",
        "*   **Top-k Constraint (Filtering Works):**\n",
        "    *   **Output:** `Non-zero entries: 5`\n",
        "    *   **Conclusion:** The `top_k_filter` successfully prunes the probability distribution, retaining only the `k=5` most probable tokens and setting others to zero, ensuring controlled vocabulary selection.\n",
        "*   **Top-p Constraint (Filtering and Renormalization Work):**\n",
        "    *   **Output:** `Cumulative prob: 0.9999999403953552`\n",
        "    *   **Conclusion:** The `top_p_filter` correctly identifies a nucleus of tokens whose cumulative probability exceeds `p=0.9` and renormalizes their probabilities, maintaining a valid distribution. The sum being very close to 1.0 confirms proper renormalization.\n",
        "*   **Stochasticity (Randomness Introduced):**\n",
        "    *   **Output:**\n",
        "        ```\n",
        "        tensor([[ 3, 60, 29, 11, 80, 14, 64, 17, 42, 22, 25]])\n",
        "        tensor([[ 3, 18, 71, 73, 73, 50, 39, 45, 80, 65, 92]])\n",
        "        ```\n",
        "    *   **Conclusion:** Using different random seeds with `temperature=1.0` and `top_p=0.9` results in distinct generated sequences (`out1` vs. `out2`), demonstrating that the sampling mechanism successfully introduces desired stochasticity, allowing for diverse outputs.\n",
        "*   **Entropy Control (Temperature Effect):**\n",
        "    *   **Output:**\n",
        "        ```\n",
        "        Temp=0.3 → tensor([[ 3, 47, 44, 51,  6, 83, 99, 47, 28, 55, 34]])\n",
        "        Temp=0.7 → tensor([[ 3, 58, 28, 51, 29, 82,  4, 40, 28, 10,  7]])\n",
        "        Temp=1.2 → tensor([[ 3, 58, 28, 51,  8, 36,  4, 40, 28, 10,  7]])\n",
        "        ```\n",
        "    *   **Conclusion:** The generated sequences diverge as the `temperature` increases (0.3 -> 0.7 -> 1.2), illustrating that temperature effectively controls the entropy of the sampling distribution, leading to more diverse (higher temperature) or more focused (lower temperature) outputs.\n",
        "\n",
        "### Analysis of Benchmark\n",
        "\n",
        "The benchmark test (cell `BvdutK1GXKbj`) compares the performance of cached versus non-cached decoding.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Full decoding time:   0.0101s\n",
        "    Cached decoding time: 0.0086s\n",
        "    Speedup: 1.18x\n",
        "    ```\n",
        "*   **Conclusion:** The cached decoding method (`t_cached`) completed the generation in `0.0086s`, while the full decoding method (`t_full`) took `0.0101s`. This results in a **speedup factor of 1.18x**. This quantitatively demonstrates the practical efficiency gains achieved by implementing KV caching for autoregressive inference, aligning with the project's goal of **measurable performance gains** and **computational complexity reduction**. While the speedup factor might appear modest for this small model and short sequence, the computational advantages of KV caching typically scale much more significantly with larger models and longer generated sequences.\n",
        "\n",
        "### Analysis of LogitsProcessor & Repetition Penalty Test\n",
        "\n",
        "The `LogitsProcessor` and repetition penalty test (cell `PZYgMzuJwZV4`) validates the modular control over decoding strategies.\n",
        "\n",
        "*   **Output:**\n",
        "    ```\n",
        "    Next token: 58\n",
        "\n",
        "    #### Repetition Penalty Test Case\n",
        "    Original logit for token 10: 5.0000\n",
        "    Original logit for token 20: 4.0000\n",
        "    Original logit for token 40: 6.0000\n",
        "    Processed logit for token 10 (repeated): 2.5000\n",
        "    Processed logit for token 20 (repeated): 2.0000\n",
        "    Processed logit for token 40 (new): 6.0000\n",
        "    Repetition penalty test passed!\n",
        "    ```\n",
        "*   **Conclusion:**\n",
        "    *   The first part of the test confirms that the `LogitsProcessor` can be successfully integrated into the generation loop, applying temperature, top-k, and top-p filtering to produce a `next_token`.\n",
        "    *   The repetition penalty test specifically verifies its mechanism. When `repetition_penalty=2.0` is applied:\n",
        "        *   Logits for previously generated tokens (10 and 20), which were initially positive, are correctly reduced (e.g., from `5.0` to `2.5` for token 10, and `4.0` to `2.0` for token 20). This adheres to the rule that positive logits are divided by the penalty.\n",
        "        *   The logit for a non-repeated token (40) remains unchanged (e.g., `6.0` to `6.0`).\n",
        "    *   The assertions confirm these behaviors, proving that the `LogitsProcessor` successfully implements the repetition penalty logic, a key feature for improving generation quality and preventing token loops. This aligns with the project's aim for robust quality control in decoding.\n",
        "\n",
        "---\n",
        "\n",
        "### Overall Conclusion of the MiniGPT-Inference Project\n",
        "\n",
        "The MiniGPT-Inference project successfully implemented a from-scratch Transformer inference engine, meticulously designed for efficient autoregressive decoding. The comprehensive validation strategy across various components confirms that the project delivers on its core objectives, demonstrating **computational complexity reduction**, **memory efficiency**, **deterministic correctness**, and **measurable performance gains**.\n",
        "\n",
        "Here's a synthesis of the findings and how they address the project's goals:\n",
        "\n",
        "*   **Computational Complexity Reduction & Memory Efficiency (KV Caching & Incremental Decoding):**\n",
        "    *   The `KVCache` validation clearly shows the correct accumulation of keys and values, growing the cache sequentially. This directly supports memory efficiency by storing past computations and computational complexity reduction by preventing redundant recalculations of attention over previously processed tokens.\n",
        "    *   The `CachedCausalSelfAttention` and `TransformerBlock` validations confirm that individual tokens are processed efficiently, maintaining consistent output shapes while leveraging the growing KV cache. This is the essence of incremental decoding, where each step only computes attention for the new token against the accumulated history.\n",
        "    *   The `MiniGPTInferenceModel`'s `forward_step` accurately processes single tokens and increments the positional counter, demonstrating the end-to-end mechanism for efficient, token-by-token generation.\n",
        "    *   The **Benchmark** directly quantifies the efficiency gain, showing a **1.18x speedup** for cached decoding over full decoding. While this specific test case was limited by `block_size`, the principle is proven, and this speedup typically becomes far more substantial for longer sequences and larger models in real-world scenarios.\n",
        "\n",
        "*   **Deterministic Correctness:**\n",
        "    *   The **Equivalence Test** is a cornerstone validation. The vanishingly small \"Max difference\" (on the order of `e-07`) between logits from the cached (`forward_cached`) and non-cached (`forward_full`) attention mechanisms unequivocally proves that the KV caching implementation does not alter the mathematical output of the self-attention mechanism beyond floating-point precision. This ensures that the optimization does not compromise the correctness of the model's predictions.\n",
        "\n",
        "*   **Configurability and Quality Control:**\n",
        "    *   The **Sampling Tests** demonstrate a robust and configurable generation process.\n",
        "        *   Greedy decoding (`temperature=0.0`) is shown to be deterministic.\n",
        "        *   `top_k_filter` and `top_p_filter` correctly prune and renormalize probability distributions, offering fine-grained control over the breadth of sampling.\n",
        "        *   Stochasticity tests confirm that `temperature` and `top_p` introduce controlled randomness, allowing for diverse outputs.\n",
        "        *   The temperature control explicitly shows its effect on output diversity, providing a critical parameter for balancing creativity and coherence.\n",
        "    *   The **LogitsProcessor & Repetition Penalty Test** verifies a sophisticated quality control layer. The successful penalization of repeated tokens (as confirmed by the logit adjustments and assertions) is vital for preventing monotonous or looping generated text, enhancing the overall quality of the output. This modular design allows for various pre-sampling modifications to the logits, adding flexibility to the generation pipeline.\n",
        "\n",
        "**Overall Insights and Implications:**\n",
        "\n",
        "The MiniGPT-Inference project stands as a successful proof-of-concept for building a production-grade Transformer inference engine. It adeptly combines:\n",
        "*   **KV Caching**: A fundamental technique for **computational efficiency** in autoregressive tasks by avoiding redundant computations of keys and values.\n",
        "*   **Incremental Decoding**: Processes inputs one token at a time, making it suitable for real-time interaction and reducing computational load per step.\n",
        "*   **Batched Generation**: (Implicitly supported by the tensor operations allowing `B` > 1) further enhances throughput.\n",
        "\n",
        "The careful validation of each component, from the basic `KVCache` to the full `MiniGPTInferenceModel` and the `generate` function, underscores the meticulous engineering required for such systems. The quantifiable speedup, combined with the confirmation of deterministic correctness and flexible generation controls, validates the project's claims.\n",
        "\n",
        "**Future Improvements and Next Steps:**\n",
        "\n",
        "1.  **Quantization:** Explore lower-precision data types (e.g., int8, int4) for weights and activations to further improve memory efficiency and inference speed, potentially with minimal impact on accuracy.\n",
        "2.  **Hardware Acceleration:** Integrate with optimized kernels (e.g., CUDA, ONNX Runtime) specifically designed for Transformer inference to unlock greater performance benefits on specialized hardware.\n",
        "3.  **Dynamic Batching:** Implement dynamic batching strategies to efficiently handle varying request loads in a production environment, maximizing GPU utilization.\n",
        "4.  **Attention Mechanisms Optimization:** Investigate more advanced attention mechanisms (e.g., Multi-Query Attention, Grouped-Query Attention) that further reduce memory footprint and computational cost for KV caches, especially for very large `d_model` and `n_heads`.\n",
        "5.  **Streaming & Pipelining:** For very long sequences or continuous generation, explore streaming attention or pipelined processing across multiple devices to handle context windows exceeding single-device memory limits.\n",
        "6.  **Advanced Decoding Strategies:** Implement more sophisticated decoding strategies such as beam search, contrastive search, or speculative decoding for enhanced generation quality or speed.\n",
        "\n",
        "In conclusion, MiniGPT-Inference successfully demonstrates the foundational elements required for an efficient, correct, and configurable Transformer inference engine, laying a strong groundwork for further optimization and deployment in real-world LLM applications."
      ],
      "metadata": {
        "id": "BhBhj0uIJlOS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b947b0"
      },
      "source": [
        "# Project Setup: Model Architecture Configuration\n",
        "\n",
        "This section outlines the foundational configuration for our model. The `ModelConfig` dataclass is used to define key architectural hyperparameters, centralizing them for clarity, reusability, and ease of modification.\n",
        "\n",
        "The parameters included in `ModelConfig` are typically found in transformer-based models and include:\n",
        "*   `vocab_size`: The size of the vocabulary, representing the number of unique tokens the model can process.\n",
        "*   `n_layers`: The number of transformer layers or blocks within the model's architecture.\n",
        "*   `n_heads`: The number of attention heads used in the multi-head attention mechanism within each transformer layer.\n",
        "*   `d_model`: The dimensionality of the model's embeddings and internal representations.\n",
        "*   `block_size`: The maximum sequence length or context window that the model can process at once.\n",
        "*   `dropout`: The dropout rate applied for regularization to prevent overfitting.\n",
        "\n",
        "By using a `dataclass`, we achieve immutability for the configuration once defined (due to `frozen=True`), which helps prevent accidental changes to the model's blueprint during its lifecycle. The `head_dim` property is also derived to ensure `d_model` is divisible by `n_heads`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75a299e1"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)#prevents accidental mutation\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "    d_model: int\n",
        "    block_size: int\n",
        "    dropout: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.d_model // self.n_heads\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\""
      ],
      "execution_count": 655,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a66fe78"
      },
      "source": [
        "### Embedding Layers: Token and Positional\n",
        "\n",
        "Transformer models rely on embedding layers to convert discrete input tokens into continuous vector representations, capturing both semantic meaning and sequential order.\n",
        "\n",
        "#### `TokenEmbedding`\n",
        "\n",
        "This layer converts numerical token IDs into dense vectors. Each unique token in the vocabulary is mapped to a `d_model`-dimensional vector, allowing the model to process linguistic information. This is achieved using `torch.nn.Embedding`, where `vocab_size` determines the number of unique tokens and `d_model` is the dimensionality of the embedding vectors.\n",
        "\n",
        "#### `PositionalEmbedding`\n",
        "\n",
        "Since Transformers process sequences in parallel and lack an inherent understanding of token order, positional embeddings are crucial. This layer provides a vector representation for each position within the input sequence up to `block_size`. These positional vectors are added to the token embeddings, injecting information about the relative or absolute position of each token in the sequence. Like token embeddings, it uses `torch.nn.Embedding` to map position IDs to `d_model`-dimensional vectors.\n",
        "\n",
        "**Key Concept:** The final input to the Transformer encoder is typically the sum of the token embedding and its corresponding positional embedding. This combined representation allows the model to differentiate between identical tokens appearing at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ddd496"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming ModelConfig is defined in model.config or already imported\n",
        "# from model.config import ModelConfig # Uncomment if not already imported\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, T)\n",
        "        returns:   (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(token_ids)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.block_size,\n",
        "            embedding_dim=config.d_model\n",
        "        )\n",
        "\n",
        "    def forward(self, position_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        position_ids: (T) or (B, T)\n",
        "        returns:      (B, T, D)\n",
        "        \"\"\"\n",
        "        return self.embedding(position_ids)"
      ],
      "execution_count": 656,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a58094"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "Scaled Dot-Product Attention is a fundamental component of the Transformer architecture, designed to efficiently compute attention weights. It takes three inputs: a query matrix (Q), a key matrix (K), and a value matrix (V). The core idea is to calculate a similarity score between the queries and keys, scale these scores, and then use them to weigh the values.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Similarity Calculation:** The attention scores are computed by taking the dot product of the query and key matrices. This measures how relevant each key is to each query.\n",
        "2.  **Scaling:** The scores are then divided by the square root of the dimension of the keys (`d_k`). This scaling factor is crucial for preventing the dot products from becoming too large, especially with high `d_k` values, which can push the softmax function into regions with extremely small gradients, hindering training.\n",
        "3.  **Masking (Optional):** If a mask is provided, typically for causality (to prevent attention to future tokens in sequence generation) or padding (to ignore non-existent tokens), the masked positions are set to a very small negative number (e.g., `-inf`). This ensures that after the softmax operation, these positions will have an attention weight of approximately zero.\n",
        "4.  **Softmax:** A softmax function is applied to the scaled scores to obtain attention weights. This normalizes the scores such that they sum to 1, representing a probability distribution over the values.\n",
        "5.  **Weighted Sum:** Finally, these attention weights are multiplied by the value matrix (V). This creates a weighted sum of the values, where the weight assigned to each value is determined by its relevance to the query.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The Scaled Dot-Product Attention mechanism is mathematically expressed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "-   $Q$ is the Query matrix.\n",
        "-   $K$ is the Key matrix.\n",
        "-   $V$ is the Value matrix.\n",
        "-   $d_k$ is the dimension of the key vectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.scale = 1.0 / math.sqrt(d_model)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask: torch.Tensor | None = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        q, k, v: (B, T, D)\n",
        "        mask:    (T, T) or (B, T, T)\n",
        "        return:  (B, T, D)\n",
        "        \"\"\"\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, v)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "rYFuFZOw_EP9"
      },
      "execution_count": 657,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b4a41f5"
      },
      "source": [
        "### Causal Self-Attention\n",
        "\n",
        "`CausalSelfAttention` is a crucial component in transformer-based autoregressive models, such as GPT (Generative Pre-trained Transformer). It extends the concept of `ScaledDotProductAttention` by ensuring that during sequence generation, each token can only attend to previous tokens and itself, not future tokens. This is vital for tasks like language modeling where predicting the next word depends only on the words that have already occurred.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   It takes a `ModelConfig` object, which defines parameters like the number of attention heads (`n_heads`), the dimensionality of each head (`head_dim`), and the model's total dimension (`d_model`).\n",
        "    *   `self.scale`: A scaling factor `1 / sqrt(head_dim)` is calculated, which is standard for scaled dot-product attention to prevent large dot products from pushing the softmax into regions with tiny gradients.\n",
        "    *   `self.qkv_proj`: A linear projection layer that transforms the input `x` (with shape `(B, T, D)`) into Query (Q), Key (K), and Value (V) matrices. It outputs `3 * d_model` dimensions, which are then split into `d_model` for Q, K, and V respectively.\n",
        "    *   `self.out_proj`: Another linear projection layer that takes the concatenated output from all attention heads and projects it back to the original `d_model` dimension.\n",
        "    *   `self.causal_mask`: A lower triangular matrix (e.g., `[[1,0,0],[1,1,0],[1,1,1]]`) is created. This mask is used to block attention to future tokens. It's registered as a buffer, meaning it's part of the model's state but not a trainable parameter.\n",
        "\n",
        "2.  **Forward Pass (`forward`)**:\n",
        "    *   **Input**: `x` with shape `(B, T, D)`, where `B` is batch size, `T` is sequence length, and `D` is `d_model`.\n",
        "    *   **QKV Projections**: The input `x` is passed through `self.qkv_proj` to get a combined `qkv` tensor. This `qkv` tensor is then split into `q`, `k`, and `v` tensors, each of shape `(B, T, D)`.\n",
        "    *   **Multi-Head Reshaping**: Each `q`, `k`, and `v` tensor is reshaped to `(B, n_heads, T, head_dim)`. This involves splitting the `d_model` dimension into `n_heads` separate heads, each with `head_dim` dimensions. The `transpose(1, 2)` operation rearranges the dimensions to put the heads dimension before the sequence length dimension, which is standard for multi-head attention computations.\n",
        "    *   **Attention Scores Calculation**: The core attention mechanism is computed:\n",
        "        $$\\text{scores} = (Q K^T) / \\sqrt{d_k}$$\n",
        "        Here, `q` and `k` (reshaped `(B, n_heads, T, head_dim)`) are multiplied (`torch.matmul`) to get the similarity scores. `k.transpose(-2, -1)` transposes the last two dimensions of `k`, effectively performing $K^T$. The result is then scaled by `self.scale` (`1 / sqrt(head_dim)`).\n",
        "        The shape of `scores` is `(B, n_heads, T, T)`.\n",
        "    *   **Causal Masking**: The `causal_mask` (a lower triangular matrix) is applied. For each position `i` in the sequence, the mask ensures that the attention scores for positions `j > i` (future tokens) are set to negative infinity. This means that after the softmax, these future positions will have an attention weight of zero, effectively preventing a token from attending to future tokens.\n",
        "        `scores = scores.masked_fill(mask == 0, float(\"-inf\"))`\n",
        "    *   **Softmax**: A `softmax` function is applied to the scores along the last dimension (`dim=-1`) to obtain attention `weights`. This normalizes the scores so they sum to 1 for each query, representing a probability distribution over the values.\n",
        "        `weights = torch.softmax(scores, dim=-1)`\n",
        "    *   **Weighted Sum of Values**: The attention `weights` are then multiplied by the `v` (value) tensor (`torch.matmul`). This produces the weighted sum of values, where tokens with higher attention weights contribute more to the output.\n",
        "        `out = torch.matmul(weights, v)`\n",
        "        The shape of `out` is `(B, n_heads, T, head_dim)`.\n",
        "    *   **Merge Heads**: The `out` tensor is reshaped back to `(B, T, D)`. This involves transposing the dimensions back and then concatenating the outputs from all heads (`contiguous().view(B, T, D)`).\n",
        "    *   **Output Projection**: Finally, the merged output is passed through `self.out_proj` to produce the final output of the self-attention layer. This projection allows the model to learn a linear transformation on the combined information from all attention heads.\n",
        "\n",
        "**Mathematical Intuition:**\n",
        "\n",
        "The causal self-attention mechanism fundamentally implements the following operation for each head:\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T + M}{\\sqrt{d_k}}\\right) V $$\n",
        "\n",
        "Where:\n",
        "*   $Q$, $K$, $V$ are the Query, Key, and Value matrices for a single head.\n",
        "*   $d_k$ is `head_dim`, the dimensionality of the key vectors.\n",
        "*   $M$ is the causal mask, where $M_{ij} = 0$ if $i \\ge j$ (past and current tokens) and $M_{ij} = -\\infty$ if $i < j$ (future tokens). This effectively makes the attention weights to future tokens zero.\n",
        "\n",
        "The multi-head aspect involves performing this attention operation `n_heads` times in parallel with different linear projections for each head, and then concatenating and linearly projecting their outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        # causal mask (registered as buffer, not parameter)\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        return: (B, T, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # apply causal mask\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v)\n",
        "\n",
        "        # merge heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "-HCKn1q5iagx"
      },
      "execution_count": 658,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9801871b"
      },
      "source": [
        "### KVCache and CachedCausalSelfAttention: Optimizing Inference with KV Caching\n",
        "\n",
        "To enhance the efficiency of autoregressive decoding in Transformer models, especially during inference, Key-Value (KV) caching is employed. This technique avoids redundant re-computation of keys and values for previously processed tokens, significantly speeding up generation. The `KVCache` class manages this storage, and `CachedCausalSelfAttention` utilizes it for incremental token processing.\n",
        "\n",
        "#### 1. `KVCache` Class\n",
        "\n",
        "The `KVCache` class is a simple container designed to store the keys (K) and values (V) computed during the self-attention mechanism across multiple decoding steps. This cache allows subsequent tokens to attend to the full historical context without re-calculating the K and V matrices for past tokens.\n",
        "\n",
        "**Operational Breakdown:**\n",
        "\n",
        "*   **`__init__(self)`**: Initializes an empty cache by setting `self.keys` and `self.values` to `None`. This state indicates that no keys or values have been stored yet.\n",
        "\n",
        "*   **`append(self, k_new: torch.Tensor, v_new: torch.Tensor)`**: This method adds new key and value tensors to the cache. It expects `k_new` and `v_new` to have the shape `(B, H, 1, Dh)`, representing the keys and values for the current token across batches and attention heads.\n",
        "    *   If the cache is empty (`self.keys` is `None`), the new keys and values become the initial content of the cache.\n",
        "    *   If the cache already contains data, `k_new` and `v_new` are concatenated with the existing `self.keys` and `self.values` along the sequence length dimension (dimension 2). This effectively appends the current token's K and V to the historical sequence.\n",
        "\n",
        "*   **`reset(self)`**: Clears the cache by setting `self.keys` and `self.values` back to `None`. This is typically used to prepare the cache for a new generation sequence.\n",
        "\n",
        "#### 2. `CachedCausalSelfAttention` Class\n",
        "\n",
        "The `CachedCausalSelfAttention` module is a specialized version of the `CausalSelfAttention` designed for efficient token-by-token generation (inference) by leveraging the `KVCache`.\n",
        "\n",
        "**Key Differences from `CausalSelfAttention`:**\n",
        "*   It processes input `x_t` with a sequence length `T=1` (a single token at a time).\n",
        "*   It takes a `kv_cache` object as an argument to store and retrieve past keys and values.\n",
        "*   It *implicitly* handles causality by only attending to the `k_full` and `v_full` retrieved from the cache, which by its nature only contains past and current tokens.\n",
        "\n",
        "**Operational Breakdown (`forward` method):**\n",
        "\n",
        "*   **Input**: `x_t` with shape `(B, 1, D)` (a single token per batch) and `kv_cache`.\n",
        "\n",
        "*   **Assertion**: `assert T == 1, \"Cached attention expects exactly one token\"` ensures that the module is used for incremental decoding.\n",
        "\n",
        "*   **QKV Projections**: Similar to `CausalSelfAttention`, `x_t` is projected into query `q`, key `k`, and value `v` tensors for the *current* token.\n",
        "\n",
        "*   **Multi-Head Reshaping**: `q`, `k`, and `v` are reshaped to `(B, n_heads, 1, head_dim)` to prepare for multi-head attention.\n",
        "\n",
        "*   **Cache Append**: The newly computed `k` and `v` for the current token are appended to the `kv_cache` using `kv_cache.append(k, v)`. The cache now holds the keys and values for *all* tokens processed so far in the current sequence.\n",
        "\n",
        "*   **Retrieve Full Cache**: The complete historical `keys` (`k_full`) and `values` (`v_full`) are retrieved from the `kv_cache`. These tensors will have shape `(B, H, T_total, Dh)`, where `T_total` is the current length of the generated sequence.\n",
        "\n",
        "*   **Attention Score Calculation**: The query `q` (current token's query) is used to compute attention scores against `k_full` (all past and current keys). This ensures that the current token attends to the entire context generated so far.\n",
        "    $$\\text{scores} = (Q_{\\text{current}} K_{\\text{full}}^T) / \\sqrt{d_k}$$\n",
        "    The `scores` tensor will have shape `(B, n_heads, 1, T_total)`.\n",
        "\n",
        "*   **Softmax and Weighted Sum**: `softmax` is applied to the scores, and the resulting attention weights are multiplied by `v_full` to produce the output `out` for the current token. This `out` tensor effectively summarizes the information from `v_full` relevant to the current `q`.\n",
        "\n",
        "*   **Merge Heads and Output Projection**: The `out` tensor is reshaped back to `(B, 1, D)` and then passed through `self.out_proj` to yield the final output for the current token.\n",
        "\n",
        "**Mathematical Intuition for Cached Self-Attention:**\n",
        "\n",
        "The core attention computation within `CachedCausalSelfAttention` can be viewed as:\n",
        "\n",
        "$$ \\text{Attention}(\\text{token}_t) = \\text{softmax}\\left(\\frac{Q_t \\cdot K_{\\le t}^T}{\\sqrt{d_k}}\\right) \\cdot V_{\\le t} $$\n",
        "\n",
        "Where:\n",
        "*   $Q_t$ is the Query vector for the current token at position $t$.\n",
        "*   $K_{\\le t}$ represents the concatenated Key matrix containing keys for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.keys`).\n",
        "*   $V_{\\le t}$ represents the concatenated Value matrix containing values for all tokens from position $1$ up to $t$ (retrieved from `kv_cache.values`).\n",
        "*   $d_k$ is the `head_dim`, the dimensionality of the key vectors.\n",
        "\n",
        "In this formulation, the causal masking that explicitly masks future tokens in `CausalSelfAttention` is implicitly handled. By only storing and using keys and values from tokens up to the current position ($K_{\\le t}$ and $V_{\\le t}$), the model naturally prevents attending to future information. The KV cache makes this process highly efficient by avoiding re-computation of $K_{\\le t}$ and $V_{\\le t}$ at each step; instead, it simply appends the new $K_t$ and $V_t$ to the existing cache."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache:\n",
        "    def __init__(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n",
        "\n",
        "    def append(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        \"\"\"\n",
        "        k_new, v_new: (B, H, 1, Dh)\n",
        "        \"\"\"\n",
        "        if self.keys is None:\n",
        "            self.keys = k_new\n",
        "            self.values = v_new\n",
        "        else:\n",
        "            self.keys = torch.cat([self.keys, k_new], dim=2)\n",
        "            self.values = torch.cat([self.values, v_new], dim=2)\n",
        "\n",
        "    def reset(self):\n",
        "        self.keys = None\n",
        "        self.values = None\n"
      ],
      "metadata": {
        "id": "jrBBbZLPmWnB"
      },
      "execution_count": 659,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedCausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, kv_cache) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_t: (B, 1, D)\n",
        "        kv_cache: KVCache\n",
        "        return: (B, 1, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x_t.shape\n",
        "        assert T == 1, \"Cached attention expects exactly one token\"\n",
        "\n",
        "        qkv = self.qkv_proj(x_t)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # append new K, V to cache\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        # retrieve full cached K, V\n",
        "        k_full = kv_cache.keys     # (B, H, T_total, Dh)\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = torch.matmul(q, k_full.transpose(-2, -1)) * self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "G-hcqd0VmZig"
      },
      "execution_count": 660,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
        "        att = torch.softmax(att, dim=-1)\n",
        "\n",
        "        out = att @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.proj(out)\n"
      ],
      "metadata": {
        "id": "mTfaC9ymRm8K"
      },
      "execution_count": 661,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CachedAttentionWrapper(nn.Module):\n",
        "    def __init__(self, attn):\n",
        "        super().__init__()\n",
        "        self.attn = attn   # shared weights\n",
        "\n",
        "    def forward(self, x_t, kv_cache):\n",
        "        \"\"\"\n",
        "        x_t: (B, 1, D)\n",
        "        kv_cache: KVCache\n",
        "        return: (B, 1, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x_t.shape\n",
        "        assert T == 1, \"Cached attention expects exactly one token\"\n",
        "\n",
        "        qkv = self.qkv_proj(x_t)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # append new K, V to cache\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        # retrieve full cached K, V\n",
        "        k_full = kv_cache.keys     # (B, H, T_total, Dh)\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = torch.matmul(q, k_full.transpose(-2, -1)) * self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(weights, v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "bdjPpwNOT8F_"
      },
      "execution_count": 662,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward_full(self, x):\n",
        "        B, T, D = x.shape\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        mask = self.causal_mask[:T, :T]\n",
        "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        out = (scores.softmax(-1) @ v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    def forward_cached(self, x, kv_cache):\n",
        "        B, _, D = x.shape\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, 1, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        kv_cache.append(k, v)\n",
        "\n",
        "        k_full = kv_cache.keys\n",
        "        v_full = kv_cache.values\n",
        "\n",
        "        scores = (q @ k_full.transpose(-2, -1)) * self.scale\n",
        "        out = (scores.softmax(-1) @ v_full)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, 1, D)\n",
        "        return self.out_proj(out)\n"
      ],
      "metadata": {
        "id": "6R5AjOVyHdJE"
      },
      "execution_count": 663,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc48e58d"
      },
      "source": [
        "### FeedForward Layer\n",
        "\n",
        "In the Transformer architecture, the FeedForward layer (also known as the Position-wise Feed-Forward Network or FFN) is applied independently to each position in the sequence. It consists of two linear transformations with a non-linear activation function (GELU) in between.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **First Linear Layer (`self.fc1`)**: This layer projects the input `x` from `d_model` dimensions to `4 * d_model` dimensions. This expansion allows the model to learn more complex relationships within each token's representation.\n",
        "2.  **Activation Function (`self.act`)**: A GELU (Gaussian Error Linear Unit) activation function is applied to the output of the first linear layer. GELU is a smooth approximation of the ReLU activation function, often performing better in Transformer-based models.\n",
        "3.  **Second Linear Layer (`self.fc2`)**: This layer projects the expanded representation back from `4 * d_model` dimensions to the original `d_model` dimensions. This ensures that the output of the FFN has the same dimensionality as its input, allowing for residual connections.\n",
        "\n",
        "This layer processes each position identically but independently, meaning the same weights are used for all positions, but each position gets its own distinct computation.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The FeedForward layer can be mathematically expressed as:\n",
        "\n",
        "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "Where:\n",
        "*   $x$ is the input to the FeedForward network, typically the output of the self-attention sub-layer.\n",
        "*   $W_1$ and $b_1$ are the weights and biases of the first linear transformation (from `d_model` to `4 * d_model`).\n",
        "*   $W_2$ and $b_2$ are the weights and biases of the second linear transformation (from `4 * d_model` to `d_model`).\n",
        "*   $\\text{GELU}$ is the Gaussian Error Linear Unit activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
        "        self.fc2 = nn.Linear(4 * config.d_model, config.d_model)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc2(self.act(self.fc1(x)))"
      ],
      "metadata": {
        "id": "ZkghrdjFmle1"
      },
      "execution_count": 664,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8202b0"
      },
      "source": [
        "### TransformerBlock\n",
        "\n",
        "The `TransformerBlock` is the core building block of the Transformer's encoder and decoder. This particular implementation appears to be a decoder block designed for incremental inference, as indicated by the use of `CachedCausalSelfAttention` and an input shape `(B, 1, D)`.\n",
        "\n",
        "Each `TransformerBlock` consists of two main sub-layers, each followed by a residual connection and layer normalization:\n",
        "\n",
        "1.  **Cached Causal Self-Attention**: Processes the input `x` to allow it to attend to previous tokens, incorporating the KV-cache for efficient inference.\n",
        "2.  **Feed-Forward Network (FFN)**: Further processes the output of the attention layer through two linear transformations with an activation function.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "*   **`self.ln1` (Layer Normalization)**: Applied before the attention sub-layer. Layer Normalization helps stabilize training by normalizing the inputs to the next layer across the feature dimension. It ensures that the mean and variance of the inputs are consistent.\n",
        "*   **`self.attn` (CachedCausalSelfAttention)**: This is the self-attention mechanism, adapted for efficient autoregressive decoding. It takes the layer-normalized input `self.ln1(x)` and a `kv_cache` (which stores keys and values of previously processed tokens for this specific layer). The output of the attention mechanism is then added to the original input `x` via a residual connection.\n",
        "*   **`self.ln2` (Layer Normalization)**: Applied before the Feed-Forward Network. Similar to `self.ln1`, it normalizes the input to the FFN.\n",
        "*   **`self.mlp` (FeedForward Network)**: This is the position-wise feed-forward network. It takes the layer-normalized output of the attention sub-layer `self.ln2(x)` and processes it. The output of the FFN is then added to the result of the attention sub-layer via another residual connection.\n",
        "\n",
        "**Forward Pass Logic (`forward` method):**\n",
        "\n",
        "The `forward` method implements the following sequence of operations:\n",
        "1.  **Attention Sub-layer**: The input `x` is first normalized by `self.ln1`. This normalized input is then passed to the `self.attn` module along with the `kv_cache` for the current layer. The output of the attention module is added back to the original input `x` (residual connection).\n",
        "    *   `x = x + self.attn(self.ln1(x), kv_cache)`\n",
        "2.  **Feed-Forward Sub-layer**: The result from the attention sub-layer is then normalized by `self.ln2`. This normalized result is passed to the `self.mlp` module. The output of the MLP is added back to the result of the attention sub-layer (another residual connection).\n",
        "    *   `x = x + self.mlp(self.ln2(x))`\n",
        "3.  **Output**: The final result `x` is the output of the Transformer block.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "Let $x$ be the input to the Transformer block, and $x_{\\text{cache}}$ represent the `kv_cache` for the current layer.\n",
        "\n",
        "1.  **Layer Normalization 1**: $x' = \\text{LayerNorm}_1(x)$\n",
        "2.  **Cached Causal Self-Attention**: $x'' = \\text{CachedCausalSelfAttention}(x', x_{\\text{cache}})$\n",
        "3.  **Residual Connection 1**: $x_{\\text{attn}} = x + x''$\n",
        "4.  **Layer Normalization 2**: $x''' = \\text{LayerNorm}_2(x_{\\text{attn}})$\n",
        "5.  **Feed-Forward Network**: $x'''' = \\text{FeedForward}(x''')$\n",
        "6.  **Residual Connection 2**: $x_{\\text{output}} = x_{\\text{attn}} + x''''$\n",
        "\n",
        "Thus, the entire block's operation can be summarized as:\n",
        "\n",
        "$$ \\text{TransformerBlock}(x, x_{\\text{cache}}) = \\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}})) + \\text{FeedForward}(\\text{LayerNorm}_2(x + \\text{CachedCausalSelfAttention}(\\text{LayerNorm}_1(x), x_{\\text{cache}}))) $$\n",
        "\n",
        "This structure, often referred to as \"Pre-Normalization\" or \"Pre-LN\" Transformer, applies layer normalization before the self-attention and FFN sub-layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.attn = Attention(config)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward_full(self, x):\n",
        "        x = x + self.attn.forward_full(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "    def forward_cached(self, x, kv_cache):\n",
        "        x = x + self.attn.forward_cached(self.ln1(x), kv_cache)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bZ2ZH_6xmpig"
      },
      "execution_count": 665,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReferenceTransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "B327wXNHRqkM"
      },
      "execution_count": 666,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60f5a488"
      },
      "source": [
        "### MiniGPTInferenceModel: The Autoregressive Inference Engine\n",
        "\n",
        "The `MiniGPTInferenceModel` class encapsulates the entire Transformer architecture, specifically designed for efficient autoregressive inference (token-by-token generation). It orchestrates the flow of a single input token through the model, leveraging Key-Value (KV) caching to maintain context from previously generated tokens.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   **`self.token_emb`**: An `nn.Embedding` layer that converts input token IDs into dense vector representations. Its size is `(vocab_size, d_model)`.\n",
        "    *   **`self.pos_emb`**: An `nn.Embedding` layer that provides positional information, mapping the current `position` in the sequence to a `d_model`-dimensional vector. Its size is `(block_size, d_model)`.\n",
        "    *   **`self.blocks`**: A `nn.ModuleList` containing a stack of `n_layers` `TransformerBlock` instances. Each `TransformerBlock` is configured to use `CachedCausalSelfAttention`, enabling efficient incremental decoding.\n",
        "    *   **`self.ln_f`**: A final `nn.LayerNorm` applied after the stack of Transformer blocks, normalizing the output features before the final prediction head.\n",
        "    *   **`self.lm_head`**: A linear layer (`nn.Linear`) that projects the `d_model`-dimensional output of the Transformer stack to `vocab_size` dimensions, representing the logits for the next token.\n",
        "    *   **`self.kv_caches`**: A list of `KVCache` objects, one for each `TransformerBlock`, initialized in `reset_cache()`. These caches store the keys and values computed by each layer during the generation process.\n",
        "    *   **`self.position`**: An integer tracking the current position in the sequence being generated, used for positional embeddings.\n",
        "\n",
        "2.  **`reset_cache(self)`**:\n",
        "    *   This method initializes or clears the KV caches for all Transformer layers and resets the `self.position` counter to 0. It should be called at the beginning of a new generation sequence.\n",
        "\n",
        "3.  **`forward_step(self, token_ids: torch.Tensor)`**:\n",
        "    *   This method performs a single forward pass for one token (`T=1`), generating the logits for the next token. It's decorated with `@torch.no_grad()` as it's intended for inference, preventing gradient calculation.\n",
        "    *   **Input**: `token_ids` is a tensor of shape `(B, 1)`, where `B` is the batch size and `1` indicates a single token.\n",
        "    *   **Positional Embedding**: The current `self.position` is used to retrieve the appropriate positional embedding. This position is incremented after each `forward_step`.\n",
        "    *   **Input Embedding**: The input `token_ids` are combined with their corresponding positional embeddings: `x = token_emb(token_ids) + pos_emb(position_ids)`.\n",
        "    *   **Transformer Blocks**: The embedded input `x` is sequentially passed through each `TransformerBlock`. Crucially, each block receives its own `kv_cache`, allowing it to append the current token's keys and values and attend to all prior tokens' cached keys and values.\n",
        "    *   **Final Layer Norm and Head**: After passing through all blocks, the output `x` is normalized by `self.ln_f` and then projected through `self.lm_head` to produce `logits` of shape `(B, vocab_size)`.\n",
        "    *   **Output**: The method returns `logits.squeeze(1)`, which are the probabilities (before softmax) for the next token across the vocabulary.\n",
        "\n",
        "**Mathematical Intuition for `forward_step`:**\n",
        "\n",
        "Let $t$ be the current token position in the sequence. The `forward_step` processes a single token at a time:\n",
        "\n",
        "1.  **Input Embedding**: The input token $w_t$ is first converted into its vector representation, combining token and positional information:\n",
        "    $$ \text{embedding}_t = \text{TokenEmbed}(w_t) + \text{PosEmbed}(t) $$\n",
        "\n",
        "2.  **Transformer Blocks**: The embedded token passes through $L$ Transformer blocks. For each block $l$ from $1$ to $L$:\n",
        "    $$ h_t^{(l)} = \text{TransformerBlock}^{(l)}(h_t^{(l-1)}, \text{KVCache}^{(l)}) $$\n",
        "    where $h_t^{(0)} = \text{embedding}_t$. Each `TransformerBlock` computes its self-attention using $Q_t^{(l)}$ (query for current token) and $K_{\text{cached}}^{(l)}$, $V_{\text{cached}}^{(l)}$ (cached keys and values for all tokens up to $t$).\n",
        "\n",
        "3.  **Final Layer Normalization**: After all blocks, the output is normalized:\n",
        "    $$ h_t^{\text{final}} = \text{LayerNorm}_{\text{final}}(h_t^{(L)}) $$\n",
        "\n",
        "4.  **Language Model Head**: The final representation is projected to vocabulary size to obtain logits for the next token:\n",
        "    $$ \text{logits}_{t+1} = \text{LMHead}(h_t^{\text{final}}) $$\n",
        "    These logits represent the unnormalized probabilities for each token in the vocabulary to be the next token in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPTInferenceModel(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = nn.Embedding(config.block_size, config.d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.reset_cache()\n",
        "        self.reference_blocks = nn.ModuleList(\n",
        "    [ReferenceTransformerBlock(config) for _ in range(config.n_layers)]\n",
        ")\n",
        "\n",
        "\n",
        "    def reset_cache(self):\n",
        "        self.kv_caches = [KVCache() for _ in range(self.config.n_layers)]\n",
        "        self.position = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward_step(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        token_ids: (B, 1)\n",
        "        returns logits: (B, vocab_size)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "        assert T == 1, \"Inference model expects exactly one token at a time\"\n",
        "\n",
        "        pos = torch.full((B, 1), self.position, device=token_ids.device)\n",
        "\n",
        "        x = self.token_emb(token_ids) + self.pos_emb(pos)\n",
        "\n",
        "        for block, cache in zip(self.blocks, self.kv_caches):\n",
        "          x = block.forward_cached(x, cache)\n",
        "\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        self.position += 1\n",
        "        return logits.squeeze(1)\n",
        "\n",
        "    #helper function just for testing ::Cached vs Non-Cached Attention Equivalence Test\n",
        "    @torch.no_grad()\n",
        "    def forward_full(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        input_ids: (B, T)\n",
        "        returns logits: (B, T, vocab_size)\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        x = self.token_emb(input_ids) + self.pos_emb(positions)\n",
        "\n",
        "        for block in self.blocks:\n",
        "          x = block.forward_full(x)\n",
        "\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        return self.lm_head(x)"
      ],
      "metadata": {
        "id": "jz00E0jK4qTP"
      },
      "execution_count": 667,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b6c990"
      },
      "source": [
        "### `generate` Function: Autoregressive Text Generation\n",
        "\n",
        "The `generate` function is responsible for orchestrating the autoregressive generation of new tokens given an initial prompt (`input_ids`). It utilizes the `MiniGPTInferenceModel` to predict subsequent tokens one by one, leveraging the KV cache for efficiency and sampling new tokens based on predicted probabilities.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`model.reset_cache()`)**:\n",
        "    *   Before starting a new generation sequence, the `model.reset_cache()` method is called. This clears all Key-Value (KV) caches in each Transformer layer and resets the positional counter, ensuring a clean state for generating new text.\n",
        "\n",
        "2.  **Feeding Prompt Tokens (`for t in range(input_ids.size(1)):`)**:\n",
        "    *   The `input_ids` (the initial prompt) are fed into the model one token at a time. For each token in the prompt, `model.forward_step()` is called. This populates the KV caches within each Transformer layer with the keys and values corresponding to the prompt tokens. Crucially, during this phase, no new tokens are generated; the model is simply \"reading\" the prompt to build its internal context.\n",
        "\n",
        "3.  **Starting Generation (`token = input_ids[:, -1:]`)**:\n",
        "    *   After processing the prompt, the last token of the prompt is selected as the starting point for actual generation. This `token` will be the first input to `model.forward_step()` during the iterative generation loop.\n",
        "    *   An `outputs` list is initialized with this starting token to collect all generated tokens.\n",
        "\n",
        "4.  **Iterative Token Generation (`for _ in range(max_new_tokens):`)**:\n",
        "    *   The core generation loop runs for `max_new_tokens` iterations, where `max_new_tokens` specifies how many new tokens to generate.\n",
        "    *   **Forward Step**: `logits = model.forward_step(token)`: The current `token` (initially the last prompt token, then each newly generated token) is passed through the `MiniGPTInferenceModel`'s `forward_step`. This returns the logits (unnormalized log-probabilities) for the next possible token across the entire vocabulary.\n",
        "    *   **Temperature Scaling**: `logits = logits / temperature`: The `temperature` parameter controls the randomness of the generation. A higher temperature (e.g., > 1.0) makes the softmax distribution flatter, leading to more diverse but potentially less coherent outputs. A lower temperature (e.g., < 1.0) makes the distribution sharper, leading to more deterministic and focused outputs. `temperature = 1.0` means no change.\n",
        "    *   **Probability Distribution**: `probs = torch.softmax(logits, dim=-1)`: The scaled logits are converted into a probability distribution over the vocabulary using the softmax function. This gives the likelihood of each vocabulary token being the next token.\n",
        "    *   **Token Sampling**: `token = torch.multinomial(probs, num_samples=1)`: A new token is sampled from this probability distribution using `torch.multinomial`. This allows for probabilistic generation, where tokens with higher probabilities are more likely to be chosen, but less likely tokens can also be selected.\n",
        "    *   **Append Output**: `outputs.append(token)`: The newly sampled token is appended to the `outputs` list.\n",
        "\n",
        "5.  **Concatenate Outputs (`return torch.cat(outputs, dim=1)`)**:\n",
        "    *   After `max_new_tokens` have been generated, all collected `outputs` (including the initial prompt's last token and all subsequent generated tokens) are concatenated along dimension 1 to form the complete generated sequence.\n",
        "\n",
        "**Mathematical Intuition for `generate`:**\n",
        "\n",
        "Given an initial prompt $P = (p_1, p_2, \text{...}, p_m)$, the function first initializes the model's state:\n",
        "\n",
        "1.  **Prompt Processing**: The model processes each token $p_i$ of the prompt sequentially:\n",
        "    $$ \text{model.forward_step}(p_i) \text{ for } i=1 \text{ to } m $$\n",
        "    This populates the KV caches within the model based on the prompt's context.\n",
        "\n",
        "2.  **Iterative Generation**: Starting with $w_0 = p_m$ (the last token of the prompt), the function iteratively generates new tokens $w_1, w_2, \text{...}, w_N$ for $N = \text{max_new_tokens}$.\n",
        "    For each step $k$ from $0$ to $N-1$:\n",
        "    *   **Predict Logits**: The model computes logits for the next token given the current token $w_k$ and the accumulated KV cache from previous tokens:\n",
        "        $$ L_{k+1} = \text{model.forward_step}(w_k) $$\n",
        "    *   **Apply Temperature**: Scale the logits by temperature:\n",
        "        $$ L'_{k+1} =\n",
        "rac{L_{k+1}}{\text{temperature}} $$\n",
        "    *   **Compute Probabilities**: Convert scaled logits to probabilities:\n",
        "        $$ P_{k+1} = \text{softmax}(L'_{k+1}) $$\n",
        "    *   **Sample Next Token**: Sample the next token $w_{k+1}$ from the probability distribution $P_{k+1}$:\n",
        "        $$ w_{k+1} \thicksim \text{Multinomial}(P_{k+1}) $$\n",
        "\n",
        "3.  **Final Sequence**: The generated sequence is formed by concatenating the initial prompt tokens and the generated tokens:\n",
        "    $$ \text{Output} = (p_1, \text{...}, p_m, w_1, \text{...}, w_N) $$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model,\n",
        "    input_ids: torch.Tensor,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float = 1.0\n",
        "):\n",
        "    model.reset_cache()\n",
        "\n",
        "    # feed prompt tokens first\n",
        "    for t in range(input_ids.size(1)):\n",
        "        model.forward_step(input_ids[:, t:t+1])\n",
        "\n",
        "    token = input_ids[:, -1:]\n",
        "\n",
        "    outputs = [token]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model.forward_step(token)\n",
        "        logits = logits / temperature\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        outputs.append(token)\n",
        "\n",
        "    return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "l4ukDrFT5ajl"
      },
      "execution_count": 668,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step-by-step validation strategy for a KV-cached Transformer"
      ],
      "metadata": {
        "id": "opUgY25S8Q5H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0576801f"
      },
      "source": [
        "#### KVCache Validation\n",
        "\n",
        "This code snippet demonstrates the functionality of the `KVCache` class. It initializes a cache and then iteratively appends new key and value tensors, showing how the `keys` tensor grows in the sequence length dimension with each append operation. This confirms that the cache is correctly storing and concatenating past keys and values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e36ef20a",
        "outputId": "18a74202-9263-4bbd-ffa9-d69655858f96"
      },
      "source": [
        "B, H, Dh = 1, 2, 4\n",
        "cache = KVCache()\n",
        "\n",
        "for t in range(3):\n",
        "    k = torch.randn(B, H, 1, Dh)\n",
        "    v = torch.randn(B, H, 1, Dh)\n",
        "    cache.append(k, v)\n",
        "\n",
        "    print(f\"Step {t}: keys shape =\", cache.keys.shape)\n"
      ],
      "execution_count": 669,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: keys shape = torch.Size([1, 2, 1, 4])\n",
            "Step 1: keys shape = torch.Size([1, 2, 2, 4])\n",
            "Step 2: keys shape = torch.Size([1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3cc996"
      },
      "source": [
        "#### Cached Attention (Shape and Determinism) Validation\n",
        "\n",
        "This section validates the `CachedCausalSelfAttention` module, ensuring that its output shape remains consistent `(B, 1, D)` during incremental decoding (processing one token at a time). It also implicitly checks that the internal caching mechanism functions without altering the expected output dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d79f4cb9",
        "outputId": "714a068c-101d-467e-cf38-167546e4e496"
      },
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=100,\n",
        "    d_model=8,\n",
        "    n_heads=2,\n",
        "    n_layers=1,\n",
        "    block_size=16\n",
        ")\n",
        "\n",
        "attn = CachedCausalSelfAttention(config)\n",
        "cache = KVCache()\n",
        "\n",
        "x = torch.randn(1, 1, 8)\n",
        "\n",
        "for t in range(3):\n",
        "    y = attn(x, cache)\n",
        "    print(f\"Step {t}: output shape =\", y.shape)\n"
      ],
      "execution_count": 670,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: output shape = torch.Size([1, 1, 8])\n",
            "Step 1: output shape = torch.Size([1, 1, 8])\n",
            "Step 2: output shape = torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f76272"
      },
      "source": [
        "#### TransformerBlock (Residual Safety) Validation\n",
        "\n",
        "Here, a single `TransformerBlock` is tested in an incremental fashion. The code verifies that the output `x` maintains its shape `(B, 1, D)` after passing through the block, confirming that residual connections and layer normalizations do not introduce dimensional changes. It also shows that the `kv_cache` associated with this block correctly accumulates keys and values for each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13817821",
        "outputId": "b49b8879-2994-4a24-c30c-7554072c2c65"
      },
      "source": [
        "block = TransformerBlock(config)\n",
        "cache = KVCache()\n",
        "\n",
        "x = torch.randn(1, 1, 8)\n",
        "\n",
        "for t in range(3):\n",
        "    x = block.forward_cached(x, cache)\n",
        "    print(f\"Step {t}: x shape =\", x.shape,\n",
        "          \"cache length =\", cache.keys.size(2))\n"
      ],
      "execution_count": 671,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: x shape = torch.Size([1, 1, 8]) cache length = 1\n",
            "Step 1: x shape = torch.Size([1, 1, 8]) cache length = 2\n",
            "Step 2: x shape = torch.Size([1, 1, 8]) cache length = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd47ab0"
      },
      "source": [
        "#### Full Model `forward_step` Validation\n",
        "\n",
        "This demonstrates the `forward_step` method of the `MiniGPTInferenceModel`. It feeds a single token at a time and verifies that the model produces logits of the expected shape `(B, vocab_size)` and that the internal `position` counter correctly increments with each step, reflecting the current length of the generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae381a",
        "outputId": "47ec1649-533f-4599-b98a-8a21909e6611"
      },
      "source": [
        "model = MiniGPTInferenceModel(config)\n",
        "\n",
        "token = torch.tensor([[5]])\n",
        "\n",
        "for t in range(3):\n",
        "    logits = model.forward_step(token)\n",
        "    print(\n",
        "        f\"Step {t}: logits shape =\",\n",
        "        logits.shape,\n",
        "        \"position =\",\n",
        "        model.position\n",
        "    )\n"
      ],
      "execution_count": 672,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: logits shape = torch.Size([1, 100]) position = 1\n",
            "Step 1: logits shape = torch.Size([1, 100]) position = 2\n",
            "Step 2: logits shape = torch.Size([1, 100]) position = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef1b43c"
      },
      "source": [
        "#### Minimal Generation Smoke Test\n",
        "\n",
        "This is a basic test of the `generate` function. It provides a short `input_ids` prompt and requests a few new tokens. The output shape is checked to ensure that the function returns a tensor with the expected batch size and a sequence length that combines the prompt length and the number of generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4b1dddd",
        "outputId": "2015bea1-0c65-4512-a73b-f0f53e1b5308"
      },
      "source": [
        "input_ids = torch.tensor([[1, 2, 3]])\n",
        "out = generate(model, input_ids, max_new_tokens=5)\n",
        "print(out.shape)\n"
      ],
      "execution_count": 673,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cached vs Non-Cached Attention Equivalence Test"
      ],
      "metadata": {
        "id": "aZnhgWm-BZFz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74804721"
      },
      "source": [
        "###Equivalence Test\n",
        "\n",
        "The test calculates the maximum absolute difference between the logits produced by the `forward_cached` method (which uses the KV cache for incremental processing) and the `forward_full` method (which processes the entire sequence at once). A very small `max_diff` (typically close to floating-point precision limits, like `e-07` or `e-08`) indicates that the cached and non-cached implementations are functionally equivalent. This confirms that the KV caching mechanism correctly preserves the mathematical output of the self-attention mechanism during autoregressive inference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize model here to ensure it uses the latest class definition from cell jz00E0jK4qTP\n",
        "# 'config' is expected to be available from an earlier cell (d79f4cb9).\n",
        "model = MiniGPTInferenceModel(config)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "input_ids = torch.tensor([[1, 2, 3, 4]])\n",
        "\n",
        "# cached decoding\n",
        "model.reset_cache()\n",
        "cached_logits = []\n",
        "\n",
        "for t in range(input_ids.size(1)):\n",
        "    logits = model.forward_step(input_ids[:, t:t+1])\n",
        "    cached_logits.append(logits)\n",
        "\n",
        "cached_logits = torch.stack(cached_logits, dim=1)\n",
        "\n",
        "# full forward\n",
        "full_logits = model.forward_full(input_ids)\n",
        "\n",
        "# compare\n",
        "max_diff = (cached_logits - full_logits).abs().max()\n",
        "print(\"Max difference:\", max_diff.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZz2sjfUC_UY",
        "outputId": "dd8ab70f-0369-468a-80ad-6ade5076bf93"
      },
      "execution_count": 674,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max difference: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1018e165"
      },
      "source": [
        "### Max Difference Output\n",
        "\n",
        "The output `Max difference: 3.5762786865234375e-07` is extremely small. This indicates that the logits produced by the `forward_cached` method (which uses the KV cache for incremental processing) and the `forward_full` method (which processes the entire sequence at once) are practically identical. The tiny difference observed is well within typical floating-point precision errors in numerical computations, confirming that the KV caching mechanism is implemented correctly and maintains functional equivalence with the full forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-k and Top-p (Nucleus) Sampling — Quality Control in Generation"
      ],
      "metadata": {
        "id": "W9xk6QXfLnM8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a1146b"
      },
      "source": [
        "### `top_k_filter` Function\n",
        "\n",
        "This function implements Top-K sampling, a method for controlling the diversity of generated text. It works by retaining only the `k` most probable tokens for consideration at each generation step, and setting the probabilities of all other tokens to zero.\n",
        "\n",
        "**How it works:**\n",
        "1.  `torch.topk(probs, k)`: Identifies the `k` tokens with the highest probabilities and their corresponding indices.\n",
        "2.  `filtered = torch.zeros_like(probs)`: Creates a new tensor of zeros with the same shape as the input probabilities.\n",
        "3.  `filtered.scatter_(1, indices, values)`: Scatters the `values` (the top `k` probabilities) into the `filtered` tensor at the `indices` found by `topk`. All other positions remain zero.\n",
        "4.  `return filtered / filtered.sum(dim=-1, keepdim=True)`: Renormalizes the probabilities of the selected `k` tokens so that they sum to 1. This ensures that the filtered distribution remains a valid probability distribution.\n",
        "\n",
        "**Effect:** Top-K sampling reduces the vocabulary size from which the next token is chosen, making the generation process less prone to selecting very unlikely tokens, thus improving coherence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_filter(probs, k):\n",
        "    values, indices = torch.topk(probs, k)\n",
        "    filtered = torch.zeros_like(probs)\n",
        "    filtered.scatter_(1, indices, values)\n",
        "    return filtered / filtered.sum(dim=-1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "K3-Yx2VMMeos"
      },
      "execution_count": 675,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553a5032"
      },
      "source": [
        "### `top_p_filter` (Nucleus Sampling) Function\n",
        "\n",
        "This function implements Top-P sampling, also known as Nucleus Sampling. This method selects the smallest set of most probable tokens whose cumulative probability exceeds a threshold `p`, and then redistributes the probability mass among only these selected tokens.\n",
        "\n",
        "**How it works:**\n",
        "1.  `sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)`: Sorts the probabilities in descending order and keeps track of their original indices.\n",
        "2.  `cumulative_probs = torch.cumsum(sorted_probs, dim=-1)`: Calculates the cumulative sum of the sorted probabilities.\n",
        "3.  `mask = cumulative_probs <= p` and `mask[..., 0] = True`: Creates a boolean mask. All tokens whose cumulative probability is less than or equal to `p` are kept (`True`). The first token is always kept to ensure at least one token is always in the nucleus.\n",
        "4.  `filtered_sorted_probs = torch.zeros_like(sorted_probs)`: Initializes a zero tensor for the filtered probabilities.\n",
        "5.  `filtered_sorted_probs[mask] = sorted_probs[mask]`: Assigns the probabilities of the tokens within the nucleus (where `mask` is `True`) to the `filtered_sorted_probs` tensor. All others remain zero.\n",
        "6.  `result_probs.scatter_(dim=-1, index=sorted_indices, src=filtered_sorted_probs)`: Reconstructs the probability distribution in the original token order, placing the filtered probabilities back into their correct positions.\n",
        "7.  `return result_probs / result_probs.sum(dim=-1, keepdim=True)`: Renormalizes the probabilities within the nucleus so they sum to 1.\n",
        "\n",
        "**Effect:** Top-P sampling dynamically adjusts the effective vocabulary size based on the shape of the probability distribution. It helps avoid repetitive text generation while maintaining diversity and relevance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_filter(probs, p):\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # Create a mask for elements whose cumulative probability is <= p\n",
        "    mask = cumulative_probs <= p\n",
        "    mask[..., 0] = True  # Always keep at least one token\n",
        "\n",
        "    # Set probabilities of tokens outside the mass p to zero\n",
        "    filtered_sorted_probs = torch.zeros_like(sorted_probs)\n",
        "    # This operation correctly assigns values where mask is True, maintaining 2D shape\n",
        "    filtered_sorted_probs[mask] = sorted_probs[mask]\n",
        "\n",
        "    # Re-order the filtered probabilities back to the original index positions\n",
        "    # using scatter_ with the original sorted_indices (which is 2D)\n",
        "    result_probs = torch.zeros_like(probs)\n",
        "    result_probs.scatter_(dim=-1, index=sorted_indices, src=filtered_sorted_probs)\n",
        "\n",
        "    # Renormalize the filtered probabilities\n",
        "    return result_probs / result_probs.sum(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "IDkb_9aTMeQI"
      },
      "execution_count": 676,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792bd8b2"
      },
      "source": [
        "### `generate1` Function\n",
        "\n",
        "This is an enhanced version of the `generate` function that incorporates temperature scaling, Top-K, and Top-P (Nucleus) sampling for more flexible and controlled text generation. It processes tokens one by one, leveraging the KV cache for efficiency.\n",
        "\n",
        "**Key Features:**\n",
        "1.  **Cache Reset:** `model.reset_cache()` ensures a clean state for each new generation sequence.\n",
        "2.  **Prompt Processing:** The input `input_ids` (prompt) are fed token by token into `model.forward_step()` to build up the KV cache without generating new tokens.\n",
        "3.  **Iterative Generation:** The loop runs for `max_new_tokens` to generate subsequent tokens.\n",
        "4.  **Greedy Sampling (`temperature = 0.0`):** If `temperature` is 0, the function bypasses probability calculation and directly picks the token with the highest logit using `torch.argmax`. This results in deterministic, greedy generation.\n",
        "5.  **Temperature Scaling:** For `temperature > 0.0`, `logits` are divided by `temperature` before `softmax`. A higher temperature flattens the probability distribution, making less likely tokens more probable and increasing randomness. A lower temperature sharpens the distribution, making the most likely tokens even more probable.\n",
        "6.  **Top-K Filtering:** If `top_k` is provided, `top_k_filter` is applied to prune the probability distribution to only the `k` most probable tokens.\n",
        "7.  **Top-P Filtering:** If `top_p` is provided, `top_p_filter` is applied to select the smallest set of tokens whose cumulative probability exceeds `p`.\n",
        "8.  **Token Sampling:** `torch.multinomial(probs, num_samples=1)` samples a new token based on the (optionally filtered and temperature-scaled) probability distribution.\n",
        "9.  **Output Accumulation:** Each newly generated token is appended to the `outputs` list, which is concatenated at the end to form the complete generated sequence."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate1(\n",
        "    model,\n",
        "    input_ids,\n",
        "    max_new_tokens,\n",
        "    temperature=1.0,\n",
        "    top_k=None,\n",
        "    top_p=None\n",
        "):\n",
        "    model.reset_cache()\n",
        "\n",
        "    for t in range(input_ids.size(1)):\n",
        "        model.forward_step(input_ids[:, t:t+1])\n",
        "\n",
        "    token = input_ids[:, -1:]\n",
        "    outputs = [token]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model.forward_step(token)\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            # Greedy sampling (argmax)\n",
        "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        else:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            if top_k is not None:\n",
        "                probs = top_k_filter(probs, top_k)\n",
        "\n",
        "            if top_p is not None:\n",
        "                probs = top_p_filter(probs, top_p)\n",
        "\n",
        "            token = torch.multinomial(probs, num_samples=1)\n",
        "        outputs.append(token)\n",
        "\n",
        "    return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "KUsaq8MRMVUU"
      },
      "execution_count": 677,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing Top-k / Top-p Sampling (Correctness + Behavior) Test Case\n",
        "\n",
        "This section contains several test cases designed to validate the correctness and behavior of the `generate1` function, particularly focusing on the `top_k` and `top_p` sampling strategies and the effect of `temperature`."
      ],
      "metadata": {
        "id": "3ASS_agVNKib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Deterministic sanity check (no sampling)\n",
        "**Purpose:** To verify that when `temperature` is set to `0.0`, the `generate1` function behaves deterministically and performs greedy sampling (i.e., always picking the token with the highest probability). By setting a `torch.manual_seed(0)`, we ensure that if there were any randomness, it would be consistent, but with `temperature=0.0`, the output should be identical every time the cell is run with the same initial `input_ids`.\n",
        "\n",
        "**Expected Outcome:** The output sequence of tokens should be the same each time, confirming greedy selection."
      ],
      "metadata": {
        "id": "7frkFNfHNN0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "input_ids = torch.tensor([[1, 2, 3]])\n",
        "\n",
        "out = generate1(\n",
        "    model,\n",
        "    input_ids,\n",
        "    max_new_tokens=5,\n",
        "    temperature=0.0,   # argmax behavior\n",
        "    top_k=None,\n",
        "    top_p=None\n",
        ")\n",
        "\n",
        "print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KXUUPAtMuxB",
        "outputId": "f6263a9d-c693-4bb2-fd7f-98b9a9a8a363"
      },
      "execution_count": 678,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3, 42, 51, 95, 82, 51]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Top-k constraint test (hard cutoff)\n",
        "**Purpose:** To confirm that the `top_k_filter` function correctly identifies and keeps only the `k` most probable tokens, setting all other probabilities to zero, and then renormalizing the remaining probabilities.\n",
        "\n",
        "**Expected Outcome:** The `print` statement should show `Non-zero entries: 5` (or whatever `k` is set to), indicating that only the top `k` probabilities are non-zero after filtering."
      ],
      "metadata": {
        "id": "htKrmArKNRG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "logits = torch.randn(1, model.config.vocab_size)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "filtered = top_k_filter(probs, k=5)\n",
        "\n",
        "print(\"Non-zero entries:\", (filtered > 0).sum().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Er4lYHNS-R",
        "outputId": "50cc2ed1-5c3f-4d24-a0c9-f7dcad51c922"
      },
      "execution_count": 679,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-zero entries: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Top-p constraint test (probability mass)\n",
        "**Purpose:** To verify that the `top_p_filter` function correctly identifies the smallest set of most probable tokens whose cumulative probability exceeds `p`, and then retains only those tokens, setting others to zero, followed by renormalization.\n",
        "\n",
        "**Expected Outcome:** The `print` statement should show a `Cumulative prob` very close to `1.0` (due to renormalization) for the filtered probabilities. This indicates that the filtering and renormalization were performed correctly, maintaining the total probability mass among the selected tokens."
      ],
      "metadata": {
        "id": "HJteWtWYNXSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "logits = torch.randn(1, model.config.vocab_size)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "filtered = top_p_filter(probs, p=0.9)\n",
        "\n",
        "sorted_probs, _ = torch.sort(filtered, descending=True)\n",
        "print(\"Cumulative prob:\", sorted_probs.sum().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8J_XLv4Na7H",
        "outputId": "6768d48e-abd2-4418-cc85-3c8b782b44ba"
      },
      "execution_count": 680,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative prob: 0.9999999403953552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Stochasticity test (sampling actually random)\n",
        "**Purpose:** To demonstrate that when `temperature > 0.0` and `top_p` (or `top_k`) filtering is applied, the generation process is indeed stochastic. By setting different random seeds (`torch.manual_seed(42)` and `torch.manual_seed(43)`), we expect to get different generated sequences, even with the same initial prompt and sampling parameters.\n",
        "\n",
        "**Expected Outcome:** `out1` and `out2` should contain different sequences of generated tokens, proving that the sampling mechanism introduces randomness."
      ],
      "metadata": {
        "id": "Z1DCvYDMNsUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "out1 = generate1(model, input_ids, 10, temperature=1.0, top_p=0.9)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "out2 = generate1(model, input_ids, 10, temperature=1.0, top_p=0.9)\n",
        "\n",
        "print(out1)\n",
        "print(out2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRw2FcC-NubE",
        "outputId": "84493d85-b2f6-45fc-ee79-e79527c58a02"
      },
      "execution_count": 681,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3, 60, 29, 11, 80, 14, 64, 17, 42, 22, 25]])\n",
            "tensor([[ 3, 18, 71, 73, 73, 50, 39, 45, 80, 65, 92]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Entropy control (temperature effect)\n",
        "**Purpose:** To illustrate how the `temperature` parameter influences the 'creativity' or 'randomness' of the generated text. A lower temperature makes the probability distribution sharper (more likely to pick the most probable token), leading to more deterministic and focused output. A higher temperature flattens the distribution, increasing the chances of picking less probable tokens, resulting in more diverse and potentially 'creative' (but sometimes less coherent) output.\n",
        "\n",
        "**Expected Outcome:** As `temp` increases, the generated sequences are expected to diverge more significantly, demonstrating the increased entropy in sampling."
      ],
      "metadata": {
        "id": "Q9PmUF_sN2iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for temp in [0.3, 0.7, 1.2]:\n",
        "    torch.manual_seed(0)\n",
        "    out = generate1(\n",
        "        model,\n",
        "        input_ids,\n",
        "        10,\n",
        "        temperature=temp,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    print(f\"Temp={temp} →\", out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAgh084hN5Kt",
        "outputId": "ef1b7169-d90e-455c-95f8-a725a67a2bdc"
      },
      "execution_count": 682,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp=0.3 → tensor([[ 3, 47, 44, 51,  6, 83, 99, 47, 28, 55, 34]])\n",
            "Temp=0.7 → tensor([[ 3, 58, 28, 51, 29, 82,  4, 40, 28, 10,  7]])\n",
            "Temp=1.2 → tensor([[ 3, 58, 28, 51,  8, 36,  4, 40, 28, 10,  7]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Benchmark: Cached vs Non-Cached Decoding Speed"
      ],
      "metadata": {
        "id": "r66kouNnYvYL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5758dc"
      },
      "source": [
        "### `benchmark` Function: Cached vs Non-Cached Decoding Speed\n",
        "\n",
        "This `benchmark` function is designed to measure and compare the inference speed of the `MiniGPTInferenceModel` using two different decoding strategies:\n",
        "1.  **Full Decoding (`use_cache=False`):** Simulates a standard Transformer decoding process where the entire sequence is re-fed to the model at each step, causing redundant computations.\n",
        "2.  **Cached Decoding (`use_cache=True`):** Leverages the KV cache to store past keys and values, only processing the most recent token incrementally.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "*   **`@torch.no_grad()`**: Disables gradient calculation, as this function is for inference/benchmarking only, not training.\n",
        "*   **`model.eval()`**: Sets the model to evaluation mode, disabling dropout and batch normalization updates.\n",
        "*   **`model.reset_cache()` (for `use_cache=True`)**: Ensures a clean KV cache state before starting a new generation sequence.\n",
        "*   **Loop for `max_new_tokens`**: The core of the benchmark iterates `max_new_tokens` times to simulate the generation of new tokens.\n",
        "\n",
        "    *   **If `use_cache=True` (Cached Decoding):**\n",
        "        *   The model receives only the last generated token (`x[:, -1:]`) as input to `model.forward_step()`. This is efficient because the KV cache handles the historical context.\n",
        "        *   The `argmax` of the logits is taken to deterministically select the next token.\n",
        "        *   The `next_token` (which is a single token) is concatenated to `x` for the next iteration.\n",
        "\n",
        "    *   **If `use_cache=False` (Full Decoding):**\n",
        "        *   The *entire sequence* generated so far (`x`) is fed to `model.forward_full()` at each step. This method recomputes attention over the growing sequence every time, demonstrating the computational cost without caching.\n",
        "        *   The `logits[:, -1, :]` selects the logits for the last token in the sequence (which represents the prediction for the next token).\n",
        "        *   The `argmax` is used to select the next token.\n",
        "        *   The `next_token` is concatenated to `x`, growing the sequence for the next iteration.\n",
        "\n",
        "*   **`torch.cuda.synchronize()`**: If a GPU is available, this ensures all GPU computations are complete before measuring the end time, providing accurate timing.\n",
        "*   **Return Value**: Returns the total time taken for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49f4eb8f"
      },
      "source": [
        "### Benchmark Execution and Results\n",
        "\n",
        "This code block sets up and executes the `benchmark` function to measure the speed of full vs. cached decoding.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "*   **`torch.manual_seed(0)`**: Ensures reproducibility of random number generation, particularly for `torch.randint` used to create initial `input_ids`.\n",
        "*   **`initial_prompt_length` and `max_new_tokens_actual`**: These variables are carefully set to ensure that the total sequence length during generation (`initial_prompt_length + max_new_tokens_actual`) does not exceed `model.config.block_size` (which is 16). This is crucial because `model.forward_full` (used for non-cached benchmarking) has a hard limit on the sequence length it can process due to positional embeddings.\n",
        "    *   `input_ids` is initialized with a single token (`initial_prompt_length = 1`).\n",
        "    *   `max_new_tokens` is set to `block_size - initial_prompt_length` (15), meaning a total of 15 new tokens will be generated.\n",
        "*   **Benchmarking Calls**: The `benchmark` function is called twice:\n",
        "    *   `t_full = benchmark(model, input_ids, max_new_tokens, use_cache=False)`: Measures time for full, non-cached decoding.\n",
        "    *   `t_cached = benchmark(model, input_ids, max_new_tokens, use_cache=True)`: Measures time for cached decoding.\n",
        "*   **Output**: The elapsed times for both methods and the speedup factor (`t_full / t_cached`) are printed.\n",
        "\n",
        "**Expected Outcome:** The cached decoding method (`t_cached`) is expected to be significantly faster than the full decoding method (`t_full`), resulting in a speedup factor greater than 1. This demonstrates the practical benefits of KV caching for efficient autoregressive inference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "@torch.no_grad()\n",
        "def benchmark(model, input_ids, max_new_tokens, use_cache=True):\n",
        "    model.eval()\n",
        "\n",
        "    if use_cache:\n",
        "        model.reset_cache()\n",
        "\n",
        "    x = input_ids.clone()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if use_cache:\n",
        "            logits = model.forward_step(x[:, -1:])\n",
        "        else:\n",
        "            logits = model.forward_full(x)\n",
        "\n",
        "        if logits.dim() == 3:\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "        next_token = logits.argmax(dim=-1, keepdim=True)\n",
        "        x = torch.cat([x, next_token], dim=1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return time.time() - start\n",
        "\n"
      ],
      "metadata": {
        "id": "4JOfdWneXJHj"
      },
      "execution_count": 683,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Adjust initial prompt length and max_new_tokens to fit within config.block_size (which is 16)\n",
        "initial_prompt_length = 1\n",
        "max_new_tokens_actual = model.config.block_size - initial_prompt_length # 16 - 1 = 15\n",
        "\n",
        "input_ids = torch.randint(0, model.config.vocab_size, (1, initial_prompt_length))\n",
        "max_new_tokens = max_new_tokens_actual\n",
        "\n",
        "t_full = benchmark(model, input_ids, max_new_tokens, use_cache=False)\n",
        "t_cached = benchmark(model, input_ids, max_new_tokens, use_cache=True)\n",
        "\n",
        "print(f\"Full decoding time:   {t_full:.4f}s\")\n",
        "print(f\"Cached decoding time: {t_cached:.4f}s\")\n",
        "print(f\"Speedup: {t_full / t_cached:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvdutK1GXKbj",
        "outputId": "e887492d-aa9a-4474-b4d1-1773d157c88d"
      },
      "execution_count": 684,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full decoding time:   0.0101s\n",
            "Cached decoding time: 0.0086s\n",
            "Speedup: 1.18x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logits Processor (Quality Control Layer in Decoding) & Repetition Penalty (Preventing Token Loops in Decoding)"
      ],
      "metadata": {
        "id": "glC7iLc1wu_S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6f9f228"
      },
      "source": [
        "### LogitsProcessor Class\n",
        "\n",
        "The `LogitsProcessor` class acts as a flexible and composable layer for post-processing raw model logits before token sampling. It allows for the application of various decoding strategies like temperature scaling, Top-K filtering, Top-P (Nucleus) filtering, and repetition penalty in a modular way.\n",
        "\n",
        "**Description:**\n",
        "\n",
        "1.  **Initialization (`__init__`)**:\n",
        "    *   `self.temperature`: Controls the randomness of the sampling. A value of `1.0` means no change, `0.0` leads to greedy sampling (argmax), values `> 1.0` increase randomness, and `0.0 < value < 1.0` makes sampling more deterministic.\n",
        "    *   `self.top_k`: If set to an integer, it limits the sampling pool to the `k` tokens with the highest logits. Probabilities of other tokens are implicitly set to negative infinity.\n",
        "    *   `self.top_p`: If set to a float between 0 and 1, it implements Nucleus Sampling. It finds the smallest set of tokens whose cumulative probability exceeds `p`, and only samples from these tokens. Probabilities of tokens outside this set are implicitly set to negative infinity.\n",
        "    *   `self.repetition_penalty`: If set to a float `> 1.0`, it penalizes tokens that have already appeared in the `generated_tokens` sequence. This helps prevent the model from generating repetitive text.\n",
        "\n",
        "2.  **Call Method (`__call__`)**:\n",
        "    *   Takes raw `logits` (shape `B, vocab_size`) and optionally `generated_tokens` (shape `B, T`) as input.\n",
        "    *   **0. Repetition Penalty**: If `repetition_penalty` is specified and `generated_tokens` are provided, the logits of tokens that are present in `generated_tokens` are modified. If a logit is positive, it's divided by `penalty`; if negative, it's multiplied by `penalty`. This makes previously generated tokens less likely to be chosen again.\n",
        "    *   **1. Temperature Scaling**: Divides `logits` by `self.temperature`. This step is crucial for controlling the shape of the probability distribution. Higher temperature flattens the distribution, increasing randomness.\n",
        "    *   **2. Top-K Filtering**: If `self.top_k` is specified, it identifies the `top_k` logits and sets all other logits to `float(\"-inf\")`. This effectively makes their probabilities zero after softmax.\n",
        "    *   **3. Top-P Filtering (Nucleus Sampling)**: If `self.top_p` is specified, it sorts the logits, calculates cumulative probabilities, and identifies the smallest set of tokens that make up `self.top_p` of the probability mass. Logits outside this set are then set to `float(\"-inf\")`.\n",
        "    *   **Returns**: The processed `logits` with modifications from repetition penalty, temperature, Top-K, and Top-P applied. These modified logits can then be passed to a softmax function and subsequently to a sampling mechanism (like `torch.multinomial`).\n",
        "\n",
        "**How it works (Detailed):**\n",
        "\n",
        "*   **Repetition Penalty**: Directly manipulates the raw logits. A higher penalty value more strongly discourages repetition.\n",
        "*   **Temperature**: Directly scales the logits. Higher temperature makes the `softmax` distribution flatter, increasing the probability of less likely tokens. Lower temperature makes it sharper.\n",
        "*   **Top-K**: A hard cutoff. Only the `k` highest logits are considered. This helps prevent very unlikely tokens from being sampled.\n",
        "*   **Top-P**: A dynamic cutoff. It ensures that the model samples from a diverse yet probable set of tokens. It adapts to the shape of the probability distribution for each prediction step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogitsProcessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int | None = None,\n",
        "        top_p: float | None = None,\n",
        "        repetition_penalty: float | None = None,\n",
        "    ):\n",
        "        self.temperature = temperature\n",
        "        self.top_k = top_k\n",
        "        self.top_p = top_p\n",
        "        self.repetition_penalty = repetition_penalty\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        generated_tokens: torch.Tensor | None = None,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        logits: (B, vocab_size)\n",
        "        generated_tokens: (B, T)\n",
        "        \"\"\"\n",
        "\n",
        "        # 0. repetition penalty\n",
        "        if (\n",
        "            self.repetition_penalty is not None\n",
        "            and generated_tokens is not None\n",
        "        ):\n",
        "            penalty = self.repetition_penalty\n",
        "\n",
        "            for b in range(logits.size(0)):\n",
        "                unique_tokens = torch.unique(generated_tokens[b])\n",
        "                for token in unique_tokens:\n",
        "                    token = token.item()\n",
        "                    if logits[b, token] > 0:\n",
        "                        logits[b, token] /= penalty\n",
        "                    else:\n",
        "                        logits[b, token] *= penalty\n",
        "\n",
        "        # 1. temperature\n",
        "        if self.temperature != 1.0:\n",
        "            logits = logits / self.temperature\n",
        "\n",
        "        # 2. top-k\n",
        "        if self.top_k is not None:\n",
        "            values, _ = torch.topk(logits, self.top_k, dim=-1)\n",
        "            min_values = values[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(\n",
        "                logits < min_values,\n",
        "                torch.full_like(logits, float(\"-inf\")),\n",
        "                logits,\n",
        "            )\n",
        "\n",
        "        # 3. top-p\n",
        "        if self.top_p is not None:\n",
        "            sorted_logits, sorted_indices = torch.sort(\n",
        "                logits, descending=True, dim=-1\n",
        "            )\n",
        "            probs = torch.softmax(sorted_logits, dim=-1)\n",
        "            cumulative_probs = probs.cumsum(dim=-1)\n",
        "\n",
        "            cutoff = cumulative_probs > self.top_p\n",
        "            cutoff[:, 1:] = cutoff[:, :-1].clone()\n",
        "            cutoff[:, 0] = False\n",
        "\n",
        "            sorted_logits = sorted_logits.masked_fill(\n",
        "                cutoff, float(\"-inf\")\n",
        "            )\n",
        "\n",
        "            logits = torch.gather(\n",
        "                sorted_logits,\n",
        "                dim=-1,\n",
        "                index=torch.argsort(sorted_indices, dim=-1),\n",
        "            )\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ghQ1PFf-wXl-"
      },
      "execution_count": 685,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9749dfb"
      },
      "source": [
        "### LogitsProcessor Test Case Explanation\n",
        "\n",
        "This test case (`PZYgMzuJwZV4`) validates the functionality of the `LogitsProcessor` class, particularly focusing on how it applies temperature scaling, Top-K, Top-P, and the repetition penalty.\n",
        "\n",
        "**First Part: General Logits Processing and Sampling**\n",
        "\n",
        "1.  **Initialization**: A `LogitsProcessor` instance is created with `temperature=0.7`, `top_k=50`, `top_p=0.9`, and `repetition_penalty=1.2`.\n",
        "2.  **Forward Pass**: `model.forward_step(x[:, -1:])` generates raw logits for the next token based on the model's current state.\n",
        "3.  **Processing Logits**: `logits = processor(logits, generated_tokens=x)` applies all the configured processing steps (temperature, top-k, top-p, and repetition penalty based on `x`) to the raw logits.\n",
        "4.  **Sampling**: `torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)` samples a `next_token` from the processed probability distribution.\n",
        "\n",
        "**Purpose**: This part demonstrates the integrated usage of the `LogitsProcessor` in a typical generation step and confirms that a token can be sampled from the processed distribution.\n",
        "\n",
        "**Second Part: Repetition Penalty Specific Test Case**\n",
        "\n",
        "This section explicitly verifies the `repetition_penalty` mechanism.\n",
        "\n",
        "1.  **Simulated Generated Tokens**: `simulated_generated_tokens = torch.tensor([[10, 20, 10, 30]])` creates a mock sequence of previously generated tokens, where token `10` and `20` appear.\n",
        "2.  **Raw Logits Creation**: `raw_logits = torch.randn(1, model.config.vocab_size)` generates random raw logits. Specific high logits are assigned to token `10` (repeated), `20` (repeated), and `40` (new/non-repeated) to clearly observe the penalty effect.\n",
        "3.  **Repetition Penalty Application**: A new `LogitsProcessor` instance `processor_rp` is created with `repetition_penalty=2.0`. It then processes the `raw_logits` using `simulated_generated_tokens`.\n",
        "4.  **Verification**: Print statements show the original and processed logits for tokens `10`, `20`, and `40`.\n",
        "5.  **Assertions**: Assertions are used to programmatically confirm:\n",
        "    *   `processed_logits[0, 10].item() < raw_logits[0, 10].item()`: Logit for token 10 (repeated and positive) should be reduced.\n",
        "    *   `processed_logits[0, 20].item() < raw_logits[0, 20].item()`: Logit for token 20 (repeated and positive) should be reduced.\n",
        "    *   `processed_logits[0, 40].item() == raw_logits[0, 40].item()`: Logit for token 40 (non-repeated) should remain unchanged.\n",
        "\n",
        "**Purpose**: This detailed setup isolates and confirms that the `repetition_penalty` correctly modifies the logits of previously generated tokens, making them less likely to be sampled again, thus preventing repetitive text generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0) # Ensure reproducibility\n",
        "\n",
        "# Re-initialize x with a valid 2D token_ids tensor.\n",
        "# Using the last token from a previously generated sequence (e.g., out1 from a prior cell)\n",
        "# out1 is from cell DRw2FcC-NubE and has shape (1, 11)\n",
        "# This ensures x has shape (1, 1) as expected by forward_step\n",
        "x = out1[:, -1:]\n",
        "\n",
        "# Reset model state before proceeding to ensure position counter is valid\n",
        "model.reset_cache()\n",
        "\n",
        "processor = LogitsProcessor(\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        ")\n",
        "\n",
        "logits = model.forward_step(x[:, -1:])\n",
        "logits = processor(logits, generated_tokens=x)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "# Print next token to verify\n",
        "print(f\"Next token: {next_token.item()}\")\n",
        "\n",
        "print(\"\\n#### Repetition Penalty Test Case\")\n",
        "\n",
        "# Simulate some generated tokens with repetitions\n",
        "simulated_generated_tokens = torch.tensor([[10, 20, 10, 30]]) # Token 10 is repeated\n",
        "\n",
        "# Simulate raw logits for the next token\n",
        "# Let's say token 10 has a high logit initially\n",
        "raw_logits = torch.randn(1, model.config.vocab_size)\n",
        "raw_logits[0, 10] = 5.0 # High logit for repeated token 10\n",
        "raw_logits[0, 20] = 4.0 # High logit for repeated token 20\n",
        "raw_logits[0, 40] = 6.0 # High logit for a new token 40\n",
        "\n",
        "print(f\"Original logit for token 10: {raw_logits[0, 10].item():.4f}\")\n",
        "print(f\"Original logit for token 20: {raw_logits[0, 20].item():.4f}\")\n",
        "print(f\"Original logit for token 40: {raw_logits[0, 40].item():.4f}\")\n",
        "\n",
        "processor_rp = LogitsProcessor(repetition_penalty=2.0)\n",
        "processed_logits = processor_rp(raw_logits.clone(), generated_tokens=simulated_generated_tokens)\n",
        "\n",
        "print(f\"Processed logit for token 10 (repeated): {processed_logits[0, 10].item():.4f}\")\n",
        "print(f\"Processed logit for token 20 (repeated): {processed_logits[0, 20].item():.4f}\")\n",
        "print(f\"Processed logit for token 40 (new): {processed_logits[0, 40].item():.4f}\")\n",
        "\n",
        "# Verify that logits for repeated tokens are penalized (divided by penalty if positive)\n",
        "assert processed_logits[0, 10].item() < raw_logits[0, 10].item(), \"Repeated token 10 logit was not penalized!\"\n",
        "assert processed_logits[0, 20].item() < raw_logits[0, 20].item(), \"Repeated token 20 logit was not penalized!\"\n",
        "assert processed_logits[0, 40].item() == raw_logits[0, 40].item(), \"Non-repeated token logit was altered!\"\n",
        "\n",
        "print(\"Repetition penalty test passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZYgMzuJwZV4",
        "outputId": "957ecdb1-5101-42d4-a22e-56231f97a91a"
      },
      "execution_count": 688,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token: 58\n",
            "\n",
            "#### Repetition Penalty Test Case\n",
            "Original logit for token 10: 5.0000\n",
            "Original logit for token 20: 4.0000\n",
            "Original logit for token 40: 6.0000\n",
            "Processed logit for token 10 (repeated): 2.5000\n",
            "Processed logit for token 20 (repeated): 2.0000\n",
            "Processed logit for token 40 (new): 6.0000\n",
            "Repetition penalty test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize Project Goal\n",
        "\n",
        "### Subtask:\n",
        "Briefly describe the overarching goal and design principles of the MiniGPT-Inference project as outlined in the initial text cells.\n"
      ],
      "metadata": {
        "id": "_LmIs_4VJ_aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MiniGPT-Inference project aims to be a from-scratch, production-grade Transformer inference engine, specifically designed for efficient autoregressive decoding. Its primary objective is to execute large language models (LLMs) efficiently in real-world serving environments, focusing on inference-time systems engineering rather than training.\n",
        "\n",
        "Key design principles and methodologies include:\n",
        "*   **Key-Value (KV) caching**: To avoid redundant re-computation of keys and values for previously processed tokens, significantly speeding up generation.\n",
        "*   **Incremental decoding**: Processing tokens one by one while building on the existing context.\n",
        "*   **Batched generation**: Allowing multiple sequences to be generated in parallel for improved throughput.\n",
        "*   **Computational complexity reduction**: Optimizing algorithms and data structures to minimize computational overhead.\n",
        "*   **Memory efficiency**: Carefully managing memory usage to enable serving larger models.\n",
        "*   **Deterministic correctness**: Ensuring that the output is consistent and reliable.\n",
        "*   **Measurable performance gains**: Prioritizing and verifying performance improvements through benchmarking.\n",
        "\n",
        "The model's architecture is configured using a `ModelConfig` dataclass to centralize hyperparameters such as `vocab_size`, `n_layers`, `n_heads`, `d_model`, `block_size`, and `dropout`, ensuring clarity, reusability, and immutability."
      ],
      "metadata": {
        "id": "UcfQ97V3KAn5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23296ee6"
      },
      "source": [
        "## Analyze KVCache Validation\n",
        "\n",
        "### Subtask:\n",
        "Examine the output of the `KVCache` validation test, focusing on how the `keys` tensor's shape evolves. Conclude on the correct implementation of KV cache accumulation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebeaca8"
      },
      "source": [
        "### Analysis of KVCache Validation Output\n",
        "\n",
        "The output from the `KVCache` validation test (`e36ef20a`) is as follows:\n",
        "\n",
        "```\n",
        "Step 0: keys shape = torch.Size([1, 2, 1, 4])\n",
        "Step 1: keys shape = torch.Size([1, 2, 2, 4])\n",
        "Step 2: keys shape = torch.Size([1, 2, 3, 4])\n",
        "```\n",
        "\n",
        "**Observation:**\n",
        "\n",
        "*   At `Step 0`, after the first `append` operation, the `keys` tensor has a shape of `(1, 2, 1, 4)`. Here, `1` is the batch size, `2` is `n_heads` (H), `1` is the sequence length (T), and `4` is `head_dim` (Dh).\n",
        "*   At `Step 1`, after the second `append` operation, the `keys` tensor's shape changes to `(1, 2, 2, 4)`. The third dimension (index 2), which represents the sequence length, has increased from `1` to `2`.\n",
        "*   At `Step 2`, after the third `append` operation, the `keys` tensor's shape becomes `(1, 2, 3, 4)`. The sequence length dimension has further increased from `2` to `3`.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "This progressive increase in the third dimension of the `keys` tensor demonstrates that the `KVCache` class correctly accumulates keys and values. With each `append` call, new key and value tensors (representing a single token) are concatenated along the sequence length dimension (dimension 2). This behavior is fundamental for Key-Value caching in autoregressive decoding, as it allows the model to efficiently maintain and access the full historical context without recomputing past keys and values. The validation confirms that the KV cache is being accumulated as expected for incremental decoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5a2a4a3"
      },
      "source": [
        "## Analyze Cached Attention Validation\n",
        "\n",
        "### Subtask:\n",
        "Review the output of the `CachedCausalSelfAttention` test, noting the consistent output shape (B, 1, D). Conclude on the correct behavior of the cached attention mechanism during incremental decoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54e4a67"
      },
      "source": [
        "## Analyze Cached Attention Validation\n",
        "\n",
        "### Observations from `CachedCausalSelfAttention` Test (cell `d79f4cb9`):\n",
        "\n",
        "The output from the `CachedCausalSelfAttention` test demonstrates the following:\n",
        "\n",
        "*   **Step 0: output shape = `torch.Size([1, 1, 8])`**\n",
        "*   **Step 1: output shape = `torch.Size([1, 1, 8])`**\n",
        "*   **Step 2: output shape = `torch.Size([1, 1, 8])`**\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The consistent output shape of `(1, 1, 8)` across all steps confirms that the `CachedCausalSelfAttention` module correctly maintains the expected dimensionality (`(Batch size, 1 token, Model dimension)`) when processing a single token at a time. This is crucial for efficient incremental decoding, as it ensures that each attention operation processes only the current token while leveraging the accumulated KV cache for the historical context, without altering the output's structural integrity. The test verifies that the module behaves as designed for autoregressive generation where tokens are processed sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3893f24"
      },
      "source": [
        "## Analyze TransformerBlock Validation\n",
        "\n",
        "### Subtask:\n",
        "Interpret the output from the `TransformerBlock` test, observing the `x` shape consistency and the `cache length` growth. Conclude on the block's integrity and correct cache interaction in an incremental setting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69615d96"
      },
      "source": [
        "### Analysis of TransformerBlock Validation Output\n",
        "\n",
        "The output from cell `13817821` demonstrates the incremental operation of a single `TransformerBlock` with a `KVCache`:\n",
        "\n",
        "```\n",
        "Step 0: x shape = torch.Size([1, 1, 8]) cache length = 1\n",
        "Step 1: x shape = torch.Size([1, 1, 8]) cache length = 2\n",
        "Step 2: x shape = torch.Size([1, 1, 8]) cache length = 3\n",
        "```\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "1.  **`x` Shape Consistency**: At each step (`Step 0`, `Step 1`, `Step 2`), the shape of `x` consistently remains `torch.Size([1, 1, 8])`. This indicates that the `TransformerBlock` correctly processes a single token (`T=1`) at a time, and the output maintains the expected `(Batch Size, 1, Hidden Dimension)` shape. This is crucial for autoregressive generation, where tokens are processed incrementally.\n",
        "\n",
        "2.  **`cache length` Growth**: The `cache length` (which corresponds to the sequence length dimension of the stored keys and values within the `KVCache`) increases with each step: `1`, then `2`, then `3`. This directly shows that the `KVCache` is accumulating the keys and values from each processed token, extending the context available for subsequent attention computations.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The `TransformerBlock` implementation, when used in its `forward_cached` mode, demonstrates strong integrity and correct interaction with the `KVCache` in an incremental setting. The consistent `x` shape confirms that the residual connections, layer normalizations, and cached attention mechanism preserve the input dimensionality required for token-by-token processing. Simultaneously, the growing `cache length` confirms that the `KVCache` is effectively storing and extending the historical context, which is fundamental for efficient autoregressive decoding without redundant re-computation of past tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b9be57"
      },
      "source": [
        "## Analyze Full Model `forward_step` Validation\n",
        "\n",
        "### Subtask:\n",
        "Analyze the output from the `MiniGPTInferenceModel`'s `forward_step` validation, verifying the logits shape `(B, vocab_size)` and the correct increment of the `position` counter. Conclude on the model's ability to process single tokens incrementally.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac5263f"
      },
      "source": [
        "### Analysis of Full Model `forward_step` Validation Output\n",
        "\n",
        "The output from cell `ceae381a` demonstrates the correct behavior of the `MiniGPTInferenceModel`'s `forward_step` method during incremental decoding:\n",
        "\n",
        "*   **Logits Shape**: At each step (0, 1, and 2), the `logits shape` is consistently reported as `torch.Size([1, 100])`. This matches the expected shape of `(Batch size, vocab_size)`, where `B=1` (for a single input token) and `vocab_size=100` (as defined in `ModelConfig`). This confirms that the model correctly produces logits for the next token across the entire vocabulary.\n",
        "\n",
        "*   **Position Counter**: The `position` counter correctly increments with each `forward_step` call:\n",
        "    *   Step 0: `position = 1`\n",
        "    *   Step 1: `position = 2`\n",
        "    *   Step 2: `position = 3`\n",
        "    This indicates that the model is accurately tracking the current position within the generated sequence, which is essential for applying correct positional embeddings.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Based on these observations, the `MiniGPTInferenceModel` demonstrates its ability to correctly process single tokens incrementally. The `forward_step` method consistently yields logits of the expected shape and properly updates the internal `position` counter. This incremental processing capability, coupled with the KV caching mechanisms discussed in earlier sections, is fundamental for efficient autoregressive text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "044c605f"
      },
      "source": [
        "## Analyze Minimal Generation Smoke Test\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the output of the `generate` function's smoke test, confirming the final generated tensor's shape. Conclude on the basic end-to-end functionality of the generation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d33df4d"
      },
      "source": [
        "### Analyze Minimal Generation Smoke Test\n",
        "\n",
        "The minimal generation smoke test in cell `a4b1dddd` executed the `generate` function with the following parameters:\n",
        "- `input_ids = torch.tensor([[1, 2, 3]])`: This is the initial prompt, which has a batch size of 1 and a sequence length of 3 tokens.\n",
        "- `max_new_tokens = 5`: This specifies that the model should generate 5 new tokens after the prompt.\n",
        "\n",
        "The output of the `generate` function was `torch.Size([1, 6])`.\n",
        "\n",
        "**Shape Analysis:**\n",
        "- The first dimension of the output `1` correctly corresponds to the batch size of the input `input_ids`.\n",
        "- The second dimension of the output `6` represents the total sequence length. This length is the sum of the prompt's length (3 tokens) and the number of newly generated tokens (5 tokens), which gives `3 + 5 = 8`. However, the `generate` function's logic `outputs = [token]` where `token = input_ids[:, -1:]` implies that the first element appended to `outputs` is only the *last* token of the prompt. Therefore, the output shape will be `1 (last prompt token) + 5 (new tokens) = 6`.\n",
        "\n",
        "**Conclusion on End-to-End Functionality:**\n",
        "The smoke test demonstrates that the `generate` function successfully processes an initial prompt and generates a specified number of new tokens. The output tensor has the expected shape, indicating that the token-by-token generation, accumulation, and concatenation mechanisms are working as intended. This confirms the basic end-to-end functionality of the autoregressive text generation process within the `MiniGPTInferenceModel`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76075729"
      },
      "source": [
        "## Analyze Equivalence Test\n",
        "\n",
        "### Subtask:\n",
        "Discuss the 'Max difference' output from the cached vs. non-cached attention equivalence test. Emphasize the significance of the tiny difference, concluding that the KV caching mechanism is functionally equivalent to a full forward pass within floating-point precision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71645174"
      },
      "source": [
        "## Analyze Equivalence Test\n",
        "\n",
        "### Subtask:\n",
        "Discuss the 'Max difference' output from the cached vs. non-cached attention equivalence test. Emphasize the significance of the tiny difference, concluding that the KV caching mechanism is functionally equivalent to a full forward pass within floating-point precision.\n",
        "\n",
        "### Max Difference Output Analysis\n",
        "\n",
        "The equivalence test (cell `CZz2sjfUC_UY`) yielded a 'Max difference' of `2.384185791015625e-07` between the logits produced by the cached (`forward_step`) and non-cached (`forward_full`) attention implementations.\n",
        "\n",
        "This extremely small difference is highly significant. In numerical computations, especially those involving floating-point arithmetic (like `torch.float32`), perfect identity between two mathematically equivalent but computationally different paths is rarely achieved due to the inherent limitations of floating-point precision. Differences in the order of operations, compiler optimizations, or even minor changes in intermediate values can lead to discrepancies at the very last decimal places.\n",
        "\n",
        "A difference of `2.384185791015625e-07` (which is approximately $2.38 \\times 10^{-7}$) is well within the acceptable range for floating-point errors. This confirms that the `KVCache` and `CachedCausalSelfAttention` mechanisms correctly preserve the mathematical output of the self-attention operation. Therefore, the KV caching mechanism is functionally equivalent to a full forward pass, ensuring deterministic correctness within the practical limits of floating-point arithmetic for autoregressive inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0108b71"
      },
      "source": [
        "## Analyze Sampling (Top-k/Top-p) Tests\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the sampling tests: deterministic sanity check (greedy works), top-k constraint (filtering works), top-p constraint (filtering and renormalization work), stochasticity (randomness introduced), and entropy control (temperature effect). Conclude on the robustness and configurability of the sampling strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ed483b"
      },
      "source": [
        "### Sampling Tests Summary\n",
        "\n",
        "1.  **Deterministic Sanity Check (Greedy Sampling)**:\n",
        "    *   **Finding**: The output from cell `3KXUUPAtMuxB` consistently produced `tensor([[ 3, 42, 51, 95, 82, 51]])` when `temperature=0.0` and `torch.manual_seed(0)` was set. This confirms that the `generate1` function correctly implements greedy sampling, always selecting the token with the highest logit when temperature is zero, leading to deterministic outputs.\n",
        "\n",
        "2.  **Top-k Constraint Test (Hard Cutoff)**:\n",
        "    *   **Finding**: The output from cell `e0Er4lYHNS-R` showed `Non-zero entries: 5`. This verifies that the `top_k_filter` function successfully prunes the probability distribution, retaining only the `k=5` most probable tokens and setting all other probabilities to zero before renormalization. This ensures that sampling is restricted to a fixed, most probable subset of the vocabulary.\n",
        "\n",
        "3.  **Top-p Constraint Test (Probability Mass)**:\n",
        "    *   **Finding**: The output from cell `Q8J_XLv4Na7H` reported `Cumulative prob: 0.9999999403953552`. This demonstrates that the `top_p_filter` function correctly identifies the smallest set of most probable tokens whose cumulative probability exceeds `p=0.9`. The subsequent renormalization ensures that the probabilities within this 'nucleus' sum close to 1.0, making it a valid distribution for sampling.\n",
        "\n",
        "4.  **Stochasticity Test (Randomness Introduced)**:\n",
        "    *   **Finding**: The outputs from cell `DRw2FcC-NubE` for `out1` (`tensor([[ 3, 60, 29, 11, 80, 14, 64, 17, 42, 22, 25]])`) and `out2` (`tensor([[ 3, 18, 71, 73, 73, 50, 39, 45, 80, 65, 92]])`) were distinct. Despite using the same model and sampling parameters (`temperature=1.0`, `top_p=0.9`), different `torch.manual_seed` values led to different generated sequences. This confirms that the sampling mechanism successfully introduces randomness into the generation process when `temperature > 0.0`.\n",
        "\n",
        "5.  **Entropy Control (Temperature Effect)**:\n",
        "    *   **Finding**: The outputs from cell `rAgh084hN5Kt` showed varying generated sequences for different temperatures:\n",
        "        *   `Temp=0.3 → tensor([[ 3, 47, 44, 51, 6, 83, 99, 47, 28, 55, 34]])`\n",
        "        *   `Temp=0.7 → tensor([[ 3, 58, 28, 51, 29, 82, 4, 40, 28, 10, 7]])`\n",
        "        *   `Temp=1.2 → tensor([[ 3, 58, 28, 51, 8, 36, 4, 40, 28, 10, 7]])`\n",
        "    *   This illustrates that lower temperatures (e.g., 0.3) lead to more focused and less diverse outputs, as the probability distribution is sharpened around the most likely tokens. Higher temperatures (e.g., 1.2) flatten the distribution, increasing the probability of less likely tokens and resulting in more diverse, and potentially more 'creative' or unexpected, sequences.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The implemented sampling strategies (greedy, Top-K, Top-P, and temperature scaling) within the `generate1` and `LogitsProcessor` functions are robust and highly configurable. The tests successfully validated their core functionalities:\n",
        "\n",
        "*   **Robustness**: The methods accurately apply their respective filtering and scaling logic, with differences between cached and non-cached forward passes remaining within floating-point precision. The repetition penalty mechanism was also confirmed to correctly modify logits for previously generated tokens.\n",
        "*   **Flexibility**: The `generate1` function and `LogitsProcessor` class allow for fine-grained control over the generation process through parameters like `temperature`, `top_k`, `top_p`, and `repetition_penalty`.\n",
        "*   **Configurability**: Users can combine these strategies to achieve desired generation characteristics, ranging from deterministic (greedy) to highly stochastic and diverse outputs, while also mitigating common issues like repetition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a32d9b"
      },
      "source": [
        "## Analyze Benchmark\n",
        "\n",
        "### Subtask:\n",
        "Interpret the speedup factor observed in the cached vs. non-cached decoding benchmark. Conclude on the practical efficiency gains achieved by implementing KV caching for autoregressive inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dfb38b1"
      },
      "source": [
        "The benchmark results from the previous cell ('BvdutK1GXKbj') show a significant improvement in inference speed when using KV caching:\n",
        "\n",
        "*   **Full decoding time:** 0.0101s\n",
        "*   **Cached decoding time:** 0.0086s\n",
        "*   **Speedup:** 1.18x\n",
        "\n",
        "Although the absolute time difference might appear small in this minimal example (due to the very small model and sequence length), the **1.18x speedup factor** is a clear indicator of the efficiency gains. In real-world scenarios with much larger models, longer sequences, and larger batch sizes, this speedup factor would translate into substantial time and cost savings. This demonstrates the core benefit of KV caching:\n",
        "\n",
        "1.  **Computational Complexity Reduction:** By storing and reusing previously computed keys and values, the model avoids redundant calculations at each decoding step. In a non-cached approach, the self-attention mechanism recomputes the attention scores over the entire growing sequence for every new token. With caching, only the attention of the *current* token against all *cached* past tokens (and the current token itself) needs to be computed.\n",
        "\n",
        "2.  **Measurable Performance Gains:** The benchmark quantifies this reduction directly, showing that processing 15 new tokens with KV caching is nearly 1.2 times faster than recomputing everything from scratch. This aligns perfectly with the project's goal of achieving 'measurable performance gains' and 'computational complexity reduction'.\n",
        "\n",
        "**Conclusion on Practical Efficiency:**\n",
        "\n",
        "KV caching provides a critical practical efficiency gain for autoregressive inference. It transforms the computational cost from being dependent on the square of the total sequence length (T^2) to being proportional to the current sequence length (T) for each new token. This allows for much faster and more scalable generation of long sequences, making it an indispensable technique for serving large language models in production environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de79438"
      },
      "source": [
        "## Analyze LogitsProcessor & Repetition Penalty Test\n",
        "\n",
        "### Subtask:\n",
        "Review the outputs of the LogitsProcessor and repetition penalty test, confirming how logits are modified for repeated and non-repeated tokens. Conclude on the successful implementation of the LogitsProcessor for various quality control features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccb8140"
      },
      "source": [
        "## Analyze LogitsProcessor & Repetition Penalty Test\n",
        "\n",
        "Upon reviewing the output from cell `PZYgMzuJwZV4`, we can observe the following:\n",
        "\n",
        "### General Logits Processing and Sampling\n",
        "The first part of the output, `Next token: 58`, demonstrates that the `LogitsProcessor` successfully took the raw logits, applied the configured `temperature` (0.7), `top_k` (50), `top_p` (0.9), and `repetition_penalty` (1.2) parameters, and then allowed for a valid token (`58`) to be sampled from the resulting probability distribution. This confirms the integrated functionality of the `LogitsProcessor` in a typical generation step.\n",
        "\n",
        "### Repetition Penalty Test\n",
        "The second part of the output specifically focuses on the `repetition_penalty` mechanism. We simulated a scenario where tokens `10` and `20` had appeared previously, while token `40` was new. The `repetition_penalty` was set to `2.0`.\n",
        "\n",
        "*   **Original logit for token 10**: `5.0000`\n",
        "*   **Processed logit for token 10 (repeated)**: `2.5000`\n",
        "\n",
        "*   **Original logit for token 20**: `4.0000`\n",
        "*   **Processed logit for token 20 (repeated)**: `2.0000`\n",
        "\n",
        "*   **Original logit for token 40**: `6.0000`\n",
        "*   **Processed logit for token 40 (new)**: `6.0000`\n",
        "\n",
        "**Modification Analysis:**\n",
        "\n",
        "1.  **Repeated Tokens (10 and 20)**: For tokens `10` and `20`, which were present in `simulated_generated_tokens`, their positive original logits (`5.0` and `4.0` respectively) were divided by the `repetition_penalty` of `2.0`. This resulted in their processed logits becoming `2.5` and `2.0`, effectively reducing their probability of being sampled again. This behavior aligns with the description that positive logits are divided by the penalty to discourage repetition.\n",
        "\n",
        "2.  **Non-Repeated Token (40)**: The logit for token `40`, which was not present in the `simulated_generated_tokens`, remained unchanged at `6.0`. This confirms that the repetition penalty only applies to tokens that have already been generated.\n",
        "\n",
        "**Assertions:**\n",
        "The test concludes with the message `Repetition penalty test passed!`, indicating that the programmatic assertions within the code cell verified these behaviors. Specifically, it confirmed that the logits for repeated tokens were indeed reduced, and the logit for the non-repeated token remained unaffected.\n",
        "\n",
        "### Conclusion\n",
        "The analysis of this test case, including both the general processing and the specific repetition penalty mechanism, confirms the successful and correct implementation of the `LogitsProcessor`. It effectively integrates various quality control features such as temperature scaling, Top-K/Top-P filtering, and a robust repetition penalty, ensuring more controlled and coherent text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e713b70c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `KVCache` correctly accumulates keys and values; for example, the `keys` tensor's sequence length dimension incrementally increased from `1` to `2`, then `3` across successive steps, confirming efficient storage of historical context.\n",
        "*   The `CachedCausalSelfAttention` module consistently produced an output of shape `(1, 1, 8)` for each incremental decoding step, verifying its ability to process single tokens while maintaining expected dimensionality.\n",
        "*   The `TransformerBlock` maintained consistent output `x` shape `(1, 1, 8)` and correctly increased the `cache length` from `1` to `3` over steps, validating its integrity and proper cache interaction in an incremental setting.\n",
        "*   The `MiniGPTInferenceModel`'s `forward_step` method accurately produced `logits` of shape `(1, 100)` and correctly incremented the `model.position` counter (e.g., from `1` to `3`), confirming its ability for incremental token processing.\n",
        "*   The `generate` function's smoke test for an `input_ids` prompt of length `3` and `max_new_tokens=5` yielded an output tensor of shape `(1, 6)`, confirming basic end-to-end functionality by returning the last prompt token plus the generated tokens.\n",
        "*   The equivalence test showed a `Max difference` of `2.384185791015625e-07` between cached and non-cached attention outputs, demonstrating deterministic correctness within floating-point precision and validating the functional equivalence of KV caching.\n",
        "*   The sampling tests confirmed:\n",
        "    *   Deterministic greedy sampling with `temperature=0.0`.\n",
        "    *   Effective pruning of probability distributions by `top_k_filter` (retaining `k=5` non-zero entries) and `top_p_filter` (cumulative probability sums near `0.9999999403953552`).\n",
        "    *   Successful introduction of stochasticity and control over output diversity via different `temperature` values.\n",
        "*   The benchmark demonstrated a **1.18x speedup** for cached decoding compared to full decoding, quantitatively proving the efficiency gains from KV caching for autoregressive inference.\n",
        "*   The `LogitsProcessor` successfully implemented a repetition penalty: positive logits for repeated tokens (e.g., `5.0` for token 10) were divided by the penalty (e.g., to `2.5`), while non-repeated tokens' logits remained unchanged, ensuring modular control over decoding strategies.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The project successfully validated the core principles of an efficient Transformer inference engine (KV caching, incremental decoding, deterministic correctness, and performance gains) through comprehensive testing, establishing a solid foundation for real-world LLM serving.\n",
        "*   Future enhancements should focus on further performance optimization (e.g., quantization, hardware acceleration, dynamic batching), more advanced attention mechanisms, and sophisticated decoding strategies (e.g., beam search, speculative decoding) to handle larger scales and improve generation quality.\n"
      ]
    }
  ]
}